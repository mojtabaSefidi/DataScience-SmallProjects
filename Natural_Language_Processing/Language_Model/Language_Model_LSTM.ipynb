{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpQUSm5CM8R7",
        "outputId": "579eb92f-6f3e-4d87-d917-9c6014fc0299"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from News.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file News/train.csv\n",
            "343500458 bytes, modified on 2020-10-21 18:37\n",
            "with a new one\n",
            "343500458 bytes, modified on 2020-10-21 18:37\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit n\n",
            "\n",
            "\n",
            "Would you like to replace the existing file News/test.csv\n",
            "62071897 bytes, modified on 2020-10-21 18:37\n",
            "with a new one\n",
            "62071897 bytes, modified on 2020-10-21 18:37\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit n\n",
            "\n",
            "All OK\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir gdown -q\n",
        "!gdown --id 1E8NPHI5lgY6RJpWau5WReG8woQnFZvXJ -q\n",
        "!unrar x News.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o8HqZw83yXmA"
      },
      "outputs": [],
      "source": [
        "from collections import Counter, OrderedDict\n",
        "import json\n",
        "import operator\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "import glob\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "from pathlib import Path\n",
        "import heapq\n",
        "import scipy\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drMPQCkLLnTL"
      },
      "source": [
        "# Auxiliary functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-U2KaTxaIGVy"
      },
      "outputs": [],
      "source": [
        "def remove(text):\n",
        "  text = text.replace('ھ','ه')\n",
        "  text = text.replace('ئ','ی')\n",
        "  text = text.replace('ؤ','و')\n",
        "  text = text.replace('إ','ا')\n",
        "  text = text.replace('أ','ا')\n",
        "  text = text.replace('َ','')\n",
        "  text = text.replace('ُ','')\n",
        "  text = text.replace('ِ','')\n",
        "  text = text.replace('ّ','')\n",
        "  text = text.replace('ء','')\n",
        "  text = text.replace('ـ','')\n",
        "  text = text.replace('،',' ')\n",
        "  text = re.sub('[^\\u0621-\\u0628\\u062A-\\u063A\\u0641-\\u0642\\u0644-\\u0648\\u064E-\\u0651\\u0655\\u067E\\u0686\\u0698\\u06A9\\u06AF\\u06BE\\u06CC\\u06F0-\\u06F9 ]', '', text)\n",
        "  return re.sub(\"[^0-9\\u0600-\\u06FF]+\", \" \", text).strip()\n",
        "  \n",
        "def control_numeric(text):\n",
        "  text = text\n",
        "  pattern_integer = r'\\d+'\n",
        "  pattern_float = \"\\d+\\.\\d+\"\n",
        "  text = re.sub(pattern_float, 'N', text)\n",
        "  text = re.sub(pattern_integer, 'N', text)\n",
        "  return text\n",
        "\n",
        "def add_tag(text):\n",
        "  string = 's ' + text\n",
        "  string = string + ' e'\n",
        "  return string\n",
        "\n",
        "def remove_nan(dataset):\n",
        "  for col in dataset.columns:\n",
        "    dataset = dataset[dataset[col].notna()]\n",
        "  return dataset\n",
        "\n",
        "\n",
        "def tokenizer(data):\n",
        "    data = [x.split(' ') for x in data]\n",
        "    return data\n",
        "\n",
        "def get_max(predictions):\n",
        "  maxs = []\n",
        "  for dc in predictions:\n",
        "    maxs.append(max(dc, key=lambda k: dc[k]))\n",
        "  return maxs\n",
        "\n",
        "def text2char(text):\n",
        "  return list(text)\n",
        "\n",
        "\n",
        "def windowing(text,length):\n",
        "  sequences = []\n",
        "  for i in range(length,len(text)):\n",
        "    sequences.append(text[i-length:i+1])\n",
        "  return sequences\n",
        "\n",
        "def encoding(sequence,char2index):\n",
        "  encoded_sequences = list()\n",
        "  for text in sequence:\n",
        "    encode_seq = [char2index[char] for char in text]\n",
        "    encoded_sequences.append(encode_seq)  \n",
        "  return encoded_sequences\n",
        "\n",
        "def label_encoder(encoded_sequences):\n",
        "  encoded_sequences = np.array(encoded_sequences)\n",
        "  return encoded_sequences[:,:-1], encoded_sequences[:,-1]\n",
        "\n",
        "\n",
        "def get_word_index(s, idx=5):\n",
        "    words = re.findall(r'\\s*\\S+\\s*', s)\n",
        "    return sum(map(len, words[:idx])) + len(words[idx]) - len(words[idx].lstrip())\n",
        "\n",
        "def dataset_labeling(data_array, batch_size, window_size=100):\n",
        "  total_batch_size = batch_size * window_size\n",
        "  n_batches = len(data_array)//total_batch_size\n",
        "  \n",
        "  data_array = data_array[:n_batches * total_batch_size]\n",
        "  data_array = data_array.reshape((batch_size, -1))\n",
        "  \n",
        "  for i in range(0, data_array.shape[1], window_size):\n",
        "      x = data_array[:, i:i+window_size]\n",
        "      y = np.zeros_like(x)\n",
        "      try:\n",
        "          y[:, :-1], y[:, -1] = x[:, 1:], data_array[:, i+window_size]\n",
        "      except IndexError:\n",
        "          y[:, :-1], y[:, -1] = x[:, 1:], data_array[:, 0]\n",
        "      yield x, y \n",
        "\n",
        "def one_hot_encode(data_array, n_labels):\n",
        "  one_hot = np.zeros((np.multiply(*data_array.shape), n_labels), dtype=np.float32)    \n",
        "  one_hot[np.arange(one_hot.shape[0]), data_array.flatten()] = 1.\n",
        "  return one_hot.reshape((*data_array.shape, n_labels))\n",
        "  \n",
        "def edit_ditsance(sentence, generated):\n",
        "    m=len(sentence)+1\n",
        "    n=len(generated)+1\n",
        "\n",
        "    tbl = {}\n",
        "    for i in range(m): tbl[i,0]=i\n",
        "    for j in range(n): tbl[0,j]=j\n",
        "    for i in range(1, m):\n",
        "        for j in range(1, n):\n",
        "            cost = 0 if sentence[i-1] == generated[j-1] else 1\n",
        "            tbl[i,j] = min(tbl[i, j-1]+1, tbl[i-1, j]+1, tbl[i-1, j-1]+cost)\n",
        "\n",
        "    return tbl[i,j]\n",
        "\n",
        "\n",
        "def calculate_CER(sentence,generated):\n",
        "  return 100 * (edit_ditsance(sentence, generated)/ len(sentence))\n",
        "\n",
        "def get_key_by_value(dictionary, char):\n",
        "  for key, value in dictionary.items():\n",
        "    if value == char:\n",
        "        return key\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIVSVaeFL-lY"
      },
      "source": [
        "## DataProcessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xFErew82GMqK"
      },
      "outputs": [],
      "source": [
        "class DataProcessor:\n",
        "  def __init__(self, \n",
        "               news_path,\n",
        "               start=0,\n",
        "               end=100):\n",
        "        self.news_path = news_path\n",
        "        self.read_data(start,end)\n",
        "        print('-------------- read_data() Done --------------')\n",
        "        self.clean_text()\n",
        "        print('-------------- clean_text() Done --------------')\n",
        "        self.count_chars()\n",
        "        print('-------------- count_chars() Done --------------')\n",
        "        self.tokenize()\n",
        "        print('-------------- tokenize() Done --------------')\n",
        "        self.encoding()\n",
        "        print('-------------- encoding() Done --------------')\n",
        "\n",
        "\n",
        "  def read_data(self,start,end):\n",
        "    data = pd.read_csv(\n",
        "        self.news_path,sep='\\t', encoding = \"utf-8-sig\",on_bad_lines='skip')[['text']]\n",
        "    self.data = data[(data.index>np.percentile(data.index, start)) & (data.index<=np.percentile(data.index, end))]\n",
        "\n",
        "  def clean_text(self):\n",
        "    data = self.data\n",
        "    data = remove_nan(data)\n",
        "    data['text'] = data['text'].apply(remove)\n",
        "    data['text'] = data['text'].apply(control_numeric)\n",
        "    data['text'] = data['text'].apply(add_tag)\n",
        "    self.data = data\n",
        "\n",
        "  def count_chars(self):\n",
        "    self.all_chars = []\n",
        "    for text in self.data['text']:\n",
        "      self.all_chars.append(text2char(text))\n",
        "    \n",
        "    flat = [x for sublist in self.all_chars for x in sublist]\n",
        "    frequencies = dict(Counter(flat))\n",
        "    frequencies = dict(sorted(frequencies.items(),key=operator.itemgetter(1),reverse=True))\n",
        "\n",
        "    self.n_unique_chars = len(frequencies.keys())\n",
        "    self.n_chars = sum(frequencies.values())\n",
        "    self.frequencies_dict = frequencies\n",
        "\n",
        "    with open('frequent.txt', 'w') as file:\n",
        "      i = 0\n",
        "      for e in frequencies.keys():\n",
        "        file.write(e + ' ' + str(frequencies[e])+ '\\n')\n",
        "        i+=1\n",
        "        if i==200:\n",
        "          break\n",
        "\n",
        "    print('Number of all characters: ',self.n_chars)\n",
        "    print('Number of unique characters: ',self.n_unique_chars)\n",
        "\n",
        "  def tokenize(self):\n",
        "    frequencies = self.frequencies_dict\n",
        "    self.index2char = {}\n",
        "    self.char2Index = {}\n",
        "    i = 0\n",
        "    \n",
        "    for char in frequencies.keys():\n",
        "      self.index2char[i] = char\n",
        "      self.char2Index[char] = i\n",
        "      i += 1\n",
        "      if i == len(frequencies):\n",
        "        break\n",
        "    \n",
        "    with open('index2Char.json', 'w') as fp:\n",
        "      json.dump(self.index2char, fp)\n",
        "    with open('char2Index.json', 'w') as fp:\n",
        "      json.dump(self.char2Index, fp)\n",
        "    \n",
        "  def encoding(self):\n",
        "    self.all_text = ''\n",
        "    for row in self.data.itertuples():\n",
        "        self.all_text = self.all_text + ' ' + str(row.text[:101])\n",
        "    self.all_text = re.sub('s+', ' ', self.all_text)\n",
        "    self.all_text.strip()\n",
        "    self.encoded_data = np.array([self.char2Index[char] for char in self.all_text])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SJ3FAd0MEOw"
      },
      "source": [
        "## LanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "meuAmFFA-ALU"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self, encoded_data, unique_chars, n_hidden=512, n_layers=4, drop_prob=0.5, lr=0.001):\n",
        "      super().__init__()\n",
        "      self.drop_prob = drop_prob\n",
        "      self.n_layers = n_layers\n",
        "      self.n_hidden = n_hidden\n",
        "      self.lr = lr\n",
        "      \n",
        "      self.unique_chars = unique_chars\n",
        "      self.index2char = dict(enumerate(self.unique_chars))\n",
        "      self.char2Index = {ch: ii for ii, ch in self.index2char.items()}\n",
        "      self.data = encoded_data\n",
        "\n",
        "  def init_weights(self):\n",
        "\n",
        "    initrange = 0.1\n",
        "    \n",
        "    self.fc.bias.data.fill_(0)\n",
        "\n",
        "    self.fc.weight.data.uniform_(-1, 1)\n",
        "\n",
        "  def forward(self, x, hc):\n",
        "      \n",
        "      ## Get x, and the new hidden state (h, c) from the lstm\n",
        "      x, (h, c) = self.lstm(x, hc)\n",
        "      \n",
        "      ## Ppass x through the dropout layer\n",
        "      x = self.dropout(x)\n",
        "      \n",
        "      # Stack up LSTM outputs using view\n",
        "      x = x.contiguous().view(x.size()[0]*x.size()[1], self.n_hidden)\n",
        "      \n",
        "      ## Put x through the fully-connected layer\n",
        "      x = self.fc(x)\n",
        "      \n",
        "      # Return x and the hidden state (h, c)\n",
        "      return x, (h, c)\n",
        "\n",
        "  def convert_to_hiddens(self, batch_size):\n",
        "    weight = next(self.parameters()).data\n",
        "    hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "              weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "    \n",
        "    return hidden\n",
        "\n",
        "  def define_model(self):\n",
        "      self.lstm = nn.LSTM(len(self.unique_chars), self.n_hidden, self.n_layers, dropout=self.drop_prob, batch_first=True)\n",
        "      self.dropout = nn.Dropout(self.drop_prob)\n",
        "      self.fc = nn.Linear(self.n_hidden, len(self.unique_chars))\n",
        "      self.init_weights()\n",
        "\n",
        "  def train_char_LSTM(self, net, data, epochs=10, batch_size=32, window_size=100, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
        "\n",
        "      net.train()\n",
        "      opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "     \n",
        "    \n",
        "      val_idx = int(len(self.data)*(1-val_frac))\n",
        "      train_dataset, validation_dataset = self.data[:val_idx], self.data[val_idx:]\n",
        "      \n",
        "      net.cuda()\n",
        "      counter = 0\n",
        "      n_chars = len(self.unique_chars)\n",
        "\n",
        "      for e in range(epochs):\n",
        "          h = self.convert_to_hiddens(batch_size)\n",
        "          \n",
        "          for x, y in dataset_labeling(train_dataset, batch_size, window_size):\n",
        "              counter += 1\n",
        "\n",
        "              x = one_hot_encode(x, n_chars)\n",
        "              inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "              inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "              data=self.data\n",
        "              h = tuple([each.data for each in h])\n",
        "\n",
        "              net.zero_grad()\n",
        "              output, h = net.forward(inputs, h)\n",
        "              \n",
        "              loss = criterion(output, targets.view(batch_size*window_size).type(torch.cuda.LongTensor))\n",
        "              loss.backward()\n",
        "\n",
        "              nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "              opt.step()\n",
        "              \n",
        "              if counter % print_every == 0:\n",
        "                  val_h = self.convert_to_hiddens(batch_size)\n",
        "                  val_losses = []\n",
        "                  net.eval()\n",
        "                  \n",
        "                  for x, y in dataset_labeling(validation_dataset, batch_size, window_size):\n",
        "                      \n",
        "                      x = one_hot_encode(x, n_chars)\n",
        "                      x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
        "                      \n",
        "                      \n",
        "                      val_h = tuple([each.data for each in h])\n",
        "                      \n",
        "                      inputs, targets = x, y\n",
        "                      inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "                      output, val_h = net.forward(inputs, val_h)\n",
        "                      val_loss = criterion(output, targets.view(batch_size*window_size).type(torch.cuda.LongTensor))\n",
        "                  \n",
        "                      val_losses.append(val_loss.item())\n",
        "                  \n",
        "                  net.train()\n",
        "                  \n",
        "                  print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                        \"Step: {}...\".format(counter),\n",
        "                        \"Loss: {:.4f}...\".format(loss.item()),\n",
        "                        \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
        "\n",
        "\n",
        "  def get_next_state_and_output(self,net, char, hidden):\n",
        "    x = np.array([[net.char2Index[char]]])\n",
        "    x = one_hot_encode(x, len(net.unique_chars))\n",
        "    X = torch.from_numpy(x)\n",
        "\n",
        "    h = tuple([each.data for each in hidden])\n",
        "    out, h = net(X.cuda(), h)\n",
        "    return out, h\n",
        "\n",
        "  def convert_prefix_to_hiddens(self,net,prefix):\n",
        "    current_hidden = self.convert_to_hiddens(batch_size=1)\n",
        "    out, h = self.get_next_state_and_output(net, prefix[0], current_hidden)\n",
        "    for char in prefix[1:]:\n",
        "      x = np.array([[net.char2Index[char]]])\n",
        "      x = one_hot_encode(x, len(net.unique_chars))\n",
        "      inputs = torch.from_numpy(x)\n",
        "      h = tuple([each.data for each in h])\n",
        "      out, h = net(inputs.cuda(), h)\n",
        "    return out, h\n",
        "\n",
        "  def get_probs(self, net, prefix):\n",
        "    out, h = self.convert_prefix_to_hiddens(net, prefix)\n",
        "    p = F.softmax(out, dim=1).data\n",
        "    keys = (p[0].cpu().numpy())\n",
        "    values = list(net.char2Index.keys())\n",
        "    return dict(zip(keys, values))    \n",
        "\n",
        "  \n",
        "  def get_next_char(self, net, prefix):\n",
        "    all_probs = self.get_probs(net, prefix)\n",
        "    max_prob = random.choice(heapq.nlargest(2, list(all_probs.keys())))\n",
        "    return all_probs[max_prob]\n",
        "\n",
        "\n",
        "  def generate_text(self, net, prefix, k):\n",
        "        char=\"\"\n",
        "        while ((char!='e') and (len(prefix) <= k)) :\n",
        "            char = self.get_next_char(net, prefix)\n",
        "            prefix += char\n",
        "        if char != 'e':\n",
        "            prefix = prefix + 'e'\n",
        "        return prefix\n",
        "                \n",
        "\n",
        "  def get_overall_prob(self, net, sentence):\n",
        "        log_prob = 0\n",
        "        for i in range(1,len(sentence)-1):\n",
        "            next_char = sentence[i+1]\n",
        "            all_probs = self.get_probs(net, sentence[:i])\n",
        "            log_prob += np.log(get_key_by_value(all_probs, next_char))\n",
        "        return log_prob\n",
        "\n",
        "\n",
        "  def evaluate(self, net, test):\n",
        "    error_rate = 0.0\n",
        "    log_likelihood = 0.0\n",
        "\n",
        "    for i in range(len(test)):\n",
        "      x = get_word_index(test['text'].iloc[i], idx=5)\n",
        "      generated_text = self.generate_text(net, test['text'].iloc[i][:x], 50)\n",
        "      error_rate += calculate_CER(test['text'].iloc[i],generated_text)    \n",
        "      log_likelihood += self.get_overall_prob(net, generated_text)/len(self.unique_chars)\n",
        "    \n",
        "    character_error_rate = (error_rate / len(test)) * 100\n",
        "    perplexity = 1 / (1 + np.exp((-1)*log_likelihood))\n",
        "    return character_error_rate, perplexity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9eV7SoUMK2n"
      },
      "source": [
        "## Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXir5-qvMuh2",
        "outputId": "9b5c2ebe-79e1-48c1-c588-920d5a58bb7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- read_data() Done --------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  63671693\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- tokenize() Done --------------\n",
            "-------------- encoding() Done --------------\n"
          ]
        }
      ],
      "source": [
        "Train_dp = DataProcessor('News/train.csv',start=0,end=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BcuyLi6ORPmd",
        "outputId": "b78213ae-53d5-412b-838b-c8ddabe2af07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LanguageModel(\n",
              "  (lstm): LSTM(37, 512, num_layers=4, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc): Linear(in_features=512, out_features=37, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "LM = LanguageModel(encoded_data=Train_dp.encoded_data, unique_chars=tuple(Train_dp.frequencies_dict.keys()))\n",
        "LM.define_model()\n",
        "LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1187d650-94db-41b3-dfc1-58cfc0f8264d",
        "id": "9ZNXko51JyzY"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/1... Step: 1... Loss: 3.6457... Val Loss: 3.1134\n",
            "Epoch: 1/1... Step: 2... Loss: 3.1510... Val Loss: 3.0380\n",
            "Epoch: 1/1... Step: 3... Loss: 3.1192... Val Loss: 2.9391\n",
            "Epoch: 1/1... Step: 4... Loss: 3.0158... Val Loss: 2.9584\n",
            "Epoch: 1/1... Step: 5... Loss: 3.0407... Val Loss: 2.9346\n",
            "Epoch: 1/1... Step: 6... Loss: 3.0069... Val Loss: 2.9010\n",
            "Epoch: 1/1... Step: 7... Loss: 2.9707... Val Loss: 2.8782\n",
            "Epoch: 1/1... Step: 8... Loss: 2.9364... Val Loss: 2.8899\n",
            "Epoch: 1/1... Step: 9... Loss: 2.9204... Val Loss: 2.8876\n",
            "Epoch: 1/1... Step: 10... Loss: 2.9353... Val Loss: 2.8730\n",
            "Epoch: 1/1... Step: 11... Loss: 2.9421... Val Loss: 2.8622\n",
            "Epoch: 1/1... Step: 12... Loss: 2.9107... Val Loss: 2.8606\n",
            "Epoch: 1/1... Step: 13... Loss: 2.9249... Val Loss: 2.8679\n",
            "Epoch: 1/1... Step: 14... Loss: 2.9448... Val Loss: 2.8724\n",
            "Epoch: 1/1... Step: 15... Loss: 2.9437... Val Loss: 2.8689\n",
            "Epoch: 1/1... Step: 16... Loss: 2.9327... Val Loss: 2.8634\n",
            "Epoch: 1/1... Step: 17... Loss: 2.9095... Val Loss: 2.8618\n",
            "Epoch: 1/1... Step: 18... Loss: 2.9314... Val Loss: 2.8627\n",
            "Epoch: 1/1... Step: 19... Loss: 2.8955... Val Loss: 2.8618\n",
            "Epoch: 1/1... Step: 20... Loss: 2.9173... Val Loss: 2.8629\n",
            "Epoch: 1/1... Step: 21... Loss: 2.9081... Val Loss: 2.8615\n",
            "Epoch: 1/1... Step: 22... Loss: 2.9179... Val Loss: 2.8553\n",
            "Epoch: 1/1... Step: 23... Loss: 2.9195... Val Loss: 2.8493\n",
            "Epoch: 1/1... Step: 24... Loss: 2.9148... Val Loss: 2.8486\n",
            "Epoch: 1/1... Step: 25... Loss: 2.9080... Val Loss: 2.8478\n",
            "Epoch: 1/1... Step: 26... Loss: 2.9227... Val Loss: 2.8427\n",
            "Epoch: 1/1... Step: 27... Loss: 2.8981... Val Loss: 2.8434\n",
            "Epoch: 1/1... Step: 28... Loss: 2.9098... Val Loss: 2.8418\n",
            "Epoch: 1/1... Step: 29... Loss: 2.8921... Val Loss: 2.8291\n",
            "Epoch: 1/1... Step: 30... Loss: 2.8989... Val Loss: 2.8208\n",
            "Epoch: 1/1... Step: 31... Loss: 2.8560... Val Loss: 2.8093\n",
            "Epoch: 1/1... Step: 32... Loss: 2.8652... Val Loss: 2.8103\n",
            "Epoch: 1/1... Step: 33... Loss: 2.8632... Val Loss: 2.8015\n",
            "Epoch: 1/1... Step: 34... Loss: 2.8509... Val Loss: 2.7897\n",
            "Epoch: 1/1... Step: 35... Loss: 2.8390... Val Loss: 2.7883\n",
            "Epoch: 1/1... Step: 36... Loss: 2.8816... Val Loss: 2.7770\n",
            "Epoch: 1/1... Step: 37... Loss: 2.8214... Val Loss: 2.7762\n",
            "Epoch: 1/1... Step: 38... Loss: 2.8028... Val Loss: 2.7602\n",
            "Epoch: 1/1... Step: 39... Loss: 2.8350... Val Loss: 2.7550\n",
            "Epoch: 1/1... Step: 40... Loss: 2.7923... Val Loss: 2.7439\n",
            "Epoch: 1/1... Step: 41... Loss: 2.7942... Val Loss: 2.7372\n",
            "Epoch: 1/1... Step: 42... Loss: 2.7727... Val Loss: 2.7341\n",
            "Epoch: 1/1... Step: 43... Loss: 2.7993... Val Loss: 2.7203\n",
            "Epoch: 1/1... Step: 44... Loss: 2.7811... Val Loss: 2.6972\n",
            "Epoch: 1/1... Step: 45... Loss: 2.7810... Val Loss: 2.7039\n",
            "Epoch: 1/1... Step: 46... Loss: 2.7735... Val Loss: 2.6982\n",
            "Epoch: 1/1... Step: 47... Loss: 2.7718... Val Loss: 2.7096\n",
            "Epoch: 1/1... Step: 48... Loss: 2.7289... Val Loss: 2.6874\n",
            "Epoch: 1/1... Step: 49... Loss: 2.7756... Val Loss: 2.6791\n",
            "Epoch: 1/1... Step: 50... Loss: 2.7314... Val Loss: 2.6669\n",
            "Epoch: 1/1... Step: 51... Loss: 2.8061... Val Loss: 2.6659\n",
            "Epoch: 1/1... Step: 52... Loss: 2.7103... Val Loss: 2.6524\n",
            "Epoch: 1/1... Step: 53... Loss: 2.7549... Val Loss: 2.6420\n",
            "Epoch: 1/1... Step: 54... Loss: 2.6928... Val Loss: 2.6389\n",
            "Epoch: 1/1... Step: 55... Loss: 2.7032... Val Loss: 2.6239\n",
            "Epoch: 1/1... Step: 56... Loss: 2.6783... Val Loss: 2.6290\n",
            "Epoch: 1/1... Step: 57... Loss: 2.7451... Val Loss: 2.6172\n",
            "Epoch: 1/1... Step: 58... Loss: 2.7071... Val Loss: 2.5936\n",
            "Epoch: 1/1... Step: 59... Loss: 2.6462... Val Loss: 2.5782\n",
            "Epoch: 1/1... Step: 60... Loss: 2.5819... Val Loss: 2.6031\n",
            "Epoch: 1/1... Step: 61... Loss: 2.5937... Val Loss: 2.5457\n",
            "Epoch: 1/1... Step: 62... Loss: 2.5890... Val Loss: 2.5572\n",
            "Epoch: 1/1... Step: 63... Loss: 2.6760... Val Loss: 2.5207\n",
            "Epoch: 1/1... Step: 64... Loss: 2.6406... Val Loss: 2.5399\n",
            "Epoch: 1/1... Step: 65... Loss: 2.5207... Val Loss: 2.5184\n",
            "Epoch: 1/1... Step: 66... Loss: 2.4813... Val Loss: 2.5179\n",
            "Epoch: 1/1... Step: 67... Loss: 2.6089... Val Loss: 2.4813\n",
            "Epoch: 1/1... Step: 68... Loss: 2.5483... Val Loss: 2.4798\n",
            "Epoch: 1/1... Step: 69... Loss: 2.5429... Val Loss: 2.4803\n",
            "Epoch: 1/1... Step: 70... Loss: 2.4984... Val Loss: 2.4709\n",
            "Epoch: 1/1... Step: 71... Loss: 2.5390... Val Loss: 2.4633\n",
            "Epoch: 1/1... Step: 72... Loss: 2.4422... Val Loss: 2.4284\n",
            "Epoch: 1/1... Step: 73... Loss: 2.5073... Val Loss: 2.4458\n",
            "Epoch: 1/1... Step: 74... Loss: 2.4591... Val Loss: 2.4978\n",
            "Epoch: 1/1... Step: 75... Loss: 2.4279... Val Loss: 2.4601\n",
            "Epoch: 1/1... Step: 76... Loss: 2.4386... Val Loss: 2.4884\n",
            "Epoch: 1/1... Step: 77... Loss: 2.3590... Val Loss: 2.3997\n",
            "Epoch: 1/1... Step: 78... Loss: 2.3684... Val Loss: 2.4200\n",
            "Epoch: 1/1... Step: 79... Loss: 2.4021... Val Loss: 2.3627\n",
            "Epoch: 1/1... Step: 80... Loss: 2.5009... Val Loss: 2.4368\n",
            "Epoch: 1/1... Step: 81... Loss: 2.3269... Val Loss: 2.4323\n",
            "Epoch: 1/1... Step: 82... Loss: 2.4534... Val Loss: 2.3781\n",
            "Epoch: 1/1... Step: 83... Loss: 2.4111... Val Loss: 2.3886\n",
            "Epoch: 1/1... Step: 84... Loss: 2.3922... Val Loss: 2.4826\n",
            "Epoch: 1/1... Step: 85... Loss: 2.3291... Val Loss: 2.4188\n",
            "Epoch: 1/1... Step: 86... Loss: 2.4038... Val Loss: 2.4118\n",
            "Epoch: 1/1... Step: 87... Loss: 2.3657... Val Loss: 2.3283\n",
            "Epoch: 1/1... Step: 88... Loss: 2.3758... Val Loss: 2.3450\n",
            "Epoch: 1/1... Step: 89... Loss: 2.3824... Val Loss: 2.3345\n",
            "Epoch: 1/1... Step: 90... Loss: 2.3937... Val Loss: 2.3691\n",
            "Epoch: 1/1... Step: 91... Loss: 2.2376... Val Loss: 2.2833\n",
            "Epoch: 1/1... Step: 92... Loss: 2.4081... Val Loss: 2.2841\n",
            "Epoch: 1/1... Step: 93... Loss: 2.2560... Val Loss: 2.3192\n",
            "Epoch: 1/1... Step: 94... Loss: 2.4166... Val Loss: 2.2889\n",
            "Epoch: 1/1... Step: 95... Loss: 2.2158... Val Loss: 2.3288\n",
            "Epoch: 1/1... Step: 96... Loss: 2.3888... Val Loss: 2.2840\n",
            "Epoch: 1/1... Step: 97... Loss: 2.2889... Val Loss: 2.2704\n",
            "Epoch: 1/1... Step: 98... Loss: 2.3163... Val Loss: 2.2643\n",
            "Epoch: 1/1... Step: 99... Loss: 2.1814... Val Loss: 2.3199\n",
            "Epoch: 1/1... Step: 100... Loss: 2.3148... Val Loss: 2.2585\n",
            "Epoch: 1/1... Step: 101... Loss: 2.1616... Val Loss: 2.3218\n",
            "Epoch: 1/1... Step: 102... Loss: 2.2856... Val Loss: 2.2191\n",
            "Epoch: 1/1... Step: 103... Loss: 2.1615... Val Loss: 2.2650\n",
            "Epoch: 1/1... Step: 104... Loss: 2.3590... Val Loss: 2.2398\n",
            "Epoch: 1/1... Step: 105... Loss: 2.2177... Val Loss: 2.2145\n",
            "Epoch: 1/1... Step: 106... Loss: 2.2554... Val Loss: 2.2241\n",
            "Epoch: 1/1... Step: 107... Loss: 2.1681... Val Loss: 2.1775\n",
            "Epoch: 1/1... Step: 108... Loss: 2.2560... Val Loss: 2.1921\n",
            "Epoch: 1/1... Step: 109... Loss: 2.1670... Val Loss: 2.2603\n",
            "Epoch: 1/1... Step: 110... Loss: 2.2067... Val Loss: 2.1677\n",
            "Epoch: 1/1... Step: 111... Loss: 2.1025... Val Loss: 2.1994\n",
            "Epoch: 1/1... Step: 112... Loss: 2.2753... Val Loss: 2.2074\n",
            "Epoch: 1/1... Step: 113... Loss: 2.2039... Val Loss: 2.2472\n",
            "Epoch: 1/1... Step: 114... Loss: 2.1313... Val Loss: 2.1445\n",
            "Epoch: 1/1... Step: 115... Loss: 2.1595... Val Loss: 2.2436\n",
            "Epoch: 1/1... Step: 116... Loss: 2.1656... Val Loss: 2.1605\n",
            "Epoch: 1/1... Step: 117... Loss: 2.1596... Val Loss: 2.2121\n",
            "Epoch: 1/1... Step: 118... Loss: 2.1647... Val Loss: 2.2580\n",
            "Epoch: 1/1... Step: 119... Loss: 2.1529... Val Loss: 2.1570\n",
            "Epoch: 1/1... Step: 120... Loss: 2.2342... Val Loss: 2.1671\n",
            "Epoch: 1/1... Step: 121... Loss: 2.1253... Val Loss: 2.1202\n",
            "Epoch: 1/1... Step: 122... Loss: 2.1348... Val Loss: 2.1227\n",
            "Epoch: 1/1... Step: 123... Loss: 2.2195... Val Loss: 2.1761\n",
            "Epoch: 1/1... Step: 124... Loss: 2.0760... Val Loss: 2.1342\n",
            "Epoch: 1/1... Step: 125... Loss: 2.1435... Val Loss: 2.1895\n",
            "Epoch: 1/1... Step: 126... Loss: 2.2578... Val Loss: 2.1335\n",
            "Epoch: 1/1... Step: 127... Loss: 2.0779... Val Loss: 2.1114\n",
            "Epoch: 1/1... Step: 128... Loss: 2.2296... Val Loss: 2.1228\n",
            "Epoch: 1/1... Step: 129... Loss: 2.1088... Val Loss: 2.2245\n",
            "Epoch: 1/1... Step: 130... Loss: 2.0908... Val Loss: 2.0969\n",
            "Epoch: 1/1... Step: 131... Loss: 2.1274... Val Loss: 2.1208\n",
            "Epoch: 1/1... Step: 132... Loss: 2.0103... Val Loss: 2.0980\n",
            "Epoch: 1/1... Step: 133... Loss: 2.1000... Val Loss: 2.0935\n",
            "Epoch: 1/1... Step: 134... Loss: 2.0369... Val Loss: 2.0844\n",
            "Epoch: 1/1... Step: 135... Loss: 2.1310... Val Loss: 2.1300\n",
            "Epoch: 1/1... Step: 136... Loss: 2.0301... Val Loss: 2.1381\n",
            "Epoch: 1/1... Step: 137... Loss: 2.1727... Val Loss: 2.0886\n",
            "Epoch: 1/1... Step: 138... Loss: 1.9513... Val Loss: 2.0842\n",
            "Epoch: 1/1... Step: 139... Loss: 2.1406... Val Loss: 2.0615\n",
            "Epoch: 1/1... Step: 140... Loss: 2.0789... Val Loss: 2.0521\n",
            "Epoch: 1/1... Step: 141... Loss: 2.0274... Val Loss: 2.1821\n",
            "Epoch: 1/1... Step: 142... Loss: 1.9736... Val Loss: 2.1225\n",
            "Epoch: 1/1... Step: 143... Loss: 2.0468... Val Loss: 2.1056\n",
            "Epoch: 1/1... Step: 144... Loss: 1.9340... Val Loss: 2.0604\n",
            "Epoch: 1/1... Step: 145... Loss: 2.0737... Val Loss: 2.0711\n",
            "Epoch: 1/1... Step: 146... Loss: 2.0004... Val Loss: 2.0416\n",
            "Epoch: 1/1... Step: 147... Loss: 2.1027... Val Loss: 2.0490\n",
            "Epoch: 1/1... Step: 148... Loss: 1.9695... Val Loss: 2.0350\n",
            "Epoch: 1/1... Step: 149... Loss: 2.1123... Val Loss: 2.0577\n",
            "Epoch: 1/1... Step: 150... Loss: 1.9317... Val Loss: 2.1090\n",
            "Epoch: 1/1... Step: 151... Loss: 2.0318... Val Loss: 1.9993\n",
            "Epoch: 1/1... Step: 152... Loss: 1.9808... Val Loss: 2.0126\n",
            "Epoch: 1/1... Step: 153... Loss: 2.2001... Val Loss: 2.0022\n",
            "Epoch: 1/1... Step: 154... Loss: 1.9095... Val Loss: 2.0495\n",
            "Epoch: 1/1... Step: 155... Loss: 1.9496... Val Loss: 2.0116\n",
            "Epoch: 1/1... Step: 156... Loss: 1.8651... Val Loss: 2.0138\n",
            "Epoch: 1/1... Step: 157... Loss: 2.1709... Val Loss: 1.9991\n",
            "Epoch: 1/1... Step: 158... Loss: 1.9019... Val Loss: 2.1484\n",
            "Epoch: 1/1... Step: 159... Loss: 2.0541... Val Loss: 1.9866\n",
            "Epoch: 1/1... Step: 160... Loss: 1.9205... Val Loss: 2.0825\n",
            "Epoch: 1/1... Step: 161... Loss: 2.1056... Val Loss: 2.0391\n",
            "Epoch: 1/1... Step: 162... Loss: 1.9148... Val Loss: 2.0287\n",
            "Epoch: 1/1... Step: 163... Loss: 2.0405... Val Loss: 1.9751\n",
            "Epoch: 1/1... Step: 164... Loss: 2.0137... Val Loss: 1.9962\n",
            "Epoch: 1/1... Step: 165... Loss: 1.9204... Val Loss: 1.9923\n",
            "Epoch: 1/1... Step: 166... Loss: 1.8026... Val Loss: 1.9790\n",
            "Epoch: 1/1... Step: 167... Loss: 1.9244... Val Loss: 1.9414\n",
            "Epoch: 1/1... Step: 168... Loss: 1.9089... Val Loss: 1.9912\n",
            "Epoch: 1/1... Step: 169... Loss: 1.9896... Val Loss: 1.9395\n",
            "Epoch: 1/1... Step: 170... Loss: 2.0991... Val Loss: 1.9477\n",
            "Epoch: 1/1... Step: 171... Loss: 1.8987... Val Loss: 1.9729\n",
            "Epoch: 1/1... Step: 172... Loss: 1.9473... Val Loss: 1.9025\n",
            "Epoch: 1/1... Step: 173... Loss: 1.9819... Val Loss: 1.9371\n",
            "Epoch: 1/1... Step: 174... Loss: 1.9369... Val Loss: 1.9395\n",
            "Epoch: 1/1... Step: 175... Loss: 1.9992... Val Loss: 1.9489\n",
            "Epoch: 1/1... Step: 176... Loss: 2.0460... Val Loss: 1.9668\n",
            "Epoch: 1/1... Step: 177... Loss: 1.8451... Val Loss: 1.9318\n",
            "Epoch: 1/1... Step: 178... Loss: 1.8976... Val Loss: 1.9549\n",
            "Epoch: 1/1... Step: 179... Loss: 1.8663... Val Loss: 1.9442\n",
            "Epoch: 1/1... Step: 180... Loss: 1.8827... Val Loss: 1.9800\n",
            "Epoch: 1/1... Step: 181... Loss: 1.9348... Val Loss: 1.9358\n",
            "Epoch: 1/1... Step: 182... Loss: 1.9119... Val Loss: 1.9588\n",
            "Epoch: 1/1... Step: 183... Loss: 1.8515... Val Loss: 1.9058\n",
            "Epoch: 1/1... Step: 184... Loss: 2.0818... Val Loss: 1.9263\n",
            "Epoch: 1/1... Step: 185... Loss: 1.9243... Val Loss: 1.9286\n",
            "Epoch: 1/1... Step: 186... Loss: 1.8736... Val Loss: 2.0541\n",
            "Epoch: 1/1... Step: 187... Loss: 1.8687... Val Loss: 1.9474\n",
            "Epoch: 1/1... Step: 188... Loss: 1.9541... Val Loss: 1.9649\n",
            "Epoch: 1/1... Step: 189... Loss: 1.9158... Val Loss: 1.9098\n",
            "Epoch: 1/1... Step: 190... Loss: 1.7764... Val Loss: 1.9393\n",
            "Epoch: 1/1... Step: 191... Loss: 1.9762... Val Loss: 1.9390\n",
            "Epoch: 1/1... Step: 192... Loss: 1.8634... Val Loss: 2.0179\n",
            "Epoch: 1/1... Step: 193... Loss: 1.8049... Val Loss: 1.9384\n",
            "Epoch: 1/1... Step: 194... Loss: 1.9095... Val Loss: 1.9051\n",
            "Epoch: 1/1... Step: 195... Loss: 1.7086... Val Loss: 1.9075\n",
            "Epoch: 1/1... Step: 196... Loss: 2.0089... Val Loss: 1.9238\n",
            "Epoch: 1/1... Step: 197... Loss: 1.9299... Val Loss: 1.8988\n",
            "Epoch: 1/1... Step: 198... Loss: 1.9817... Val Loss: 1.9104\n",
            "Epoch: 1/1... Step: 199... Loss: 1.8666... Val Loss: 1.8822\n",
            "Epoch: 1/1... Step: 200... Loss: 2.0519... Val Loss: 1.8982\n",
            "Epoch: 1/1... Step: 201... Loss: 2.0187... Val Loss: 1.8999\n",
            "Epoch: 1/1... Step: 202... Loss: 1.9909... Val Loss: 1.8721\n",
            "Epoch: 1/1... Step: 203... Loss: 1.7321... Val Loss: 1.9268\n",
            "Epoch: 1/1... Step: 204... Loss: 1.9743... Val Loss: 1.8967\n",
            "Epoch: 1/1... Step: 205... Loss: 1.9477... Val Loss: 1.8276\n",
            "Epoch: 1/1... Step: 206... Loss: 2.1116... Val Loss: 1.8753\n",
            "Epoch: 1/1... Step: 207... Loss: 1.7573... Val Loss: 1.8637\n",
            "Epoch: 1/1... Step: 208... Loss: 1.9422... Val Loss: 1.9067\n",
            "Epoch: 1/1... Step: 209... Loss: 1.7722... Val Loss: 1.9069\n",
            "Epoch: 1/1... Step: 210... Loss: 1.8926... Val Loss: 1.8475\n",
            "Epoch: 1/1... Step: 211... Loss: 1.7137... Val Loss: 1.8803\n",
            "Epoch: 1/1... Step: 212... Loss: 1.8266... Val Loss: 1.8520\n",
            "Epoch: 1/1... Step: 213... Loss: 1.5536... Val Loss: 1.9360\n",
            "Epoch: 1/1... Step: 214... Loss: 1.8357... Val Loss: 1.8907\n",
            "Epoch: 1/1... Step: 215... Loss: 1.7838... Val Loss: 1.9843\n",
            "Epoch: 1/1... Step: 216... Loss: 1.9202... Val Loss: 1.8730\n",
            "Epoch: 1/1... Step: 217... Loss: 1.8505... Val Loss: 1.9210\n",
            "Epoch: 1/1... Step: 218... Loss: 1.8137... Val Loss: 1.8911\n",
            "Epoch: 1/1... Step: 219... Loss: 1.8114... Val Loss: 1.8872\n",
            "Epoch: 1/1... Step: 220... Loss: 1.7644... Val Loss: 1.9004\n",
            "Epoch: 1/1... Step: 221... Loss: 1.7847... Val Loss: 1.8892\n",
            "Epoch: 1/1... Step: 222... Loss: 1.9332... Val Loss: 1.8779\n",
            "Epoch: 1/1... Step: 223... Loss: 1.7284... Val Loss: 1.9219\n",
            "Epoch: 1/1... Step: 224... Loss: 1.8119... Val Loss: 1.8521\n",
            "Epoch: 1/1... Step: 225... Loss: 1.8739... Val Loss: 1.8761\n",
            "Epoch: 1/1... Step: 226... Loss: 1.8136... Val Loss: 1.8633\n",
            "Epoch: 1/1... Step: 227... Loss: 1.7714... Val Loss: 2.0117\n",
            "Epoch: 1/1... Step: 228... Loss: 1.7423... Val Loss: 1.9024\n",
            "Epoch: 1/1... Step: 229... Loss: 1.9337... Val Loss: 1.9124\n",
            "Epoch: 1/1... Step: 230... Loss: 1.6527... Val Loss: 1.8696\n",
            "Epoch: 1/1... Step: 231... Loss: 1.7207... Val Loss: 1.8467\n",
            "Epoch: 1/1... Step: 232... Loss: 1.7630... Val Loss: 1.8251\n",
            "Epoch: 1/1... Step: 233... Loss: 1.7407... Val Loss: 1.9269\n",
            "Epoch: 1/1... Step: 234... Loss: 1.7380... Val Loss: 1.8336\n",
            "Epoch: 1/1... Step: 235... Loss: 1.8535... Val Loss: 1.9264\n",
            "Epoch: 1/1... Step: 236... Loss: 1.7278... Val Loss: 1.8266\n",
            "Epoch: 1/1... Step: 237... Loss: 1.6994... Val Loss: 1.9272\n",
            "Epoch: 1/1... Step: 238... Loss: 1.7706... Val Loss: 1.8229\n",
            "Epoch: 1/1... Step: 239... Loss: 1.8371... Val Loss: 1.9223\n",
            "Epoch: 1/1... Step: 240... Loss: 1.7174... Val Loss: 1.9108\n",
            "Epoch: 1/1... Step: 241... Loss: 1.8331... Val Loss: 1.9712\n",
            "Epoch: 1/1... Step: 242... Loss: 1.7666... Val Loss: 1.8761\n",
            "Epoch: 1/1... Step: 243... Loss: 1.9079... Val Loss: 1.8911\n",
            "Epoch: 1/1... Step: 244... Loss: 1.6494... Val Loss: 1.8325\n",
            "Epoch: 1/1... Step: 245... Loss: 1.8537... Val Loss: 1.8622\n",
            "Epoch: 1/1... Step: 246... Loss: 1.6476... Val Loss: 1.8685\n",
            "Epoch: 1/1... Step: 247... Loss: 1.9304... Val Loss: 1.9259\n",
            "Epoch: 1/1... Step: 248... Loss: 1.5945... Val Loss: 1.8850\n",
            "Epoch: 1/1... Step: 249... Loss: 1.8786... Val Loss: 1.9178\n",
            "Epoch: 1/1... Step: 250... Loss: 1.7200... Val Loss: 1.8591\n",
            "Epoch: 1/1... Step: 251... Loss: 1.7357... Val Loss: 1.8306\n",
            "Epoch: 1/1... Step: 252... Loss: 1.7482... Val Loss: 1.8227\n",
            "Epoch: 1/1... Step: 253... Loss: 1.6787... Val Loss: 1.8898\n",
            "Epoch: 1/1... Step: 254... Loss: 1.7371... Val Loss: 1.8568\n",
            "Epoch: 1/1... Step: 255... Loss: 1.7076... Val Loss: 1.8361\n",
            "Epoch: 1/1... Step: 256... Loss: 1.5738... Val Loss: 1.8320\n",
            "Epoch: 1/1... Step: 257... Loss: 1.8722... Val Loss: 1.8305\n",
            "Epoch: 1/1... Step: 258... Loss: 1.6201... Val Loss: 1.8028\n",
            "Epoch: 1/1... Step: 259... Loss: 1.9486... Val Loss: 1.8274\n",
            "Epoch: 1/1... Step: 260... Loss: 1.6928... Val Loss: 1.8525\n",
            "Epoch: 1/1... Step: 261... Loss: 1.8171... Val Loss: 1.8042\n",
            "Epoch: 1/1... Step: 262... Loss: 1.6189... Val Loss: 1.8479\n",
            "Epoch: 1/1... Step: 263... Loss: 1.8441... Val Loss: 1.8052\n",
            "Epoch: 1/1... Step: 264... Loss: 1.8249... Val Loss: 1.8378\n",
            "Epoch: 1/1... Step: 265... Loss: 1.7362... Val Loss: 1.8354\n",
            "Epoch: 1/1... Step: 266... Loss: 1.6092... Val Loss: 1.8577\n",
            "Epoch: 1/1... Step: 267... Loss: 1.7690... Val Loss: 1.8410\n",
            "Epoch: 1/1... Step: 268... Loss: 1.5761... Val Loss: 1.8526\n",
            "Epoch: 1/1... Step: 269... Loss: 1.7475... Val Loss: 1.8409\n",
            "Epoch: 1/1... Step: 270... Loss: 1.6133... Val Loss: 1.8283\n",
            "Epoch: 1/1... Step: 271... Loss: 1.7490... Val Loss: 1.8263\n",
            "Epoch: 1/1... Step: 272... Loss: 1.6997... Val Loss: 1.8310\n",
            "Epoch: 1/1... Step: 273... Loss: 1.7253... Val Loss: 1.8169\n",
            "Epoch: 1/1... Step: 274... Loss: 1.6734... Val Loss: 1.8183\n",
            "Epoch: 1/1... Step: 275... Loss: 1.6070... Val Loss: 1.8397\n",
            "Epoch: 1/1... Step: 276... Loss: 1.7477... Val Loss: 1.8424\n",
            "Epoch: 1/1... Step: 277... Loss: 1.7859... Val Loss: 1.8417\n",
            "Epoch: 1/1... Step: 278... Loss: 1.7178... Val Loss: 1.8523\n",
            "Epoch: 1/1... Step: 279... Loss: 1.7657... Val Loss: 1.7760\n",
            "Epoch: 1/1... Step: 280... Loss: 1.8082... Val Loss: 1.7736\n",
            "Epoch: 1/1... Step: 281... Loss: 1.7698... Val Loss: 1.7993\n",
            "Epoch: 1/1... Step: 282... Loss: 1.6736... Val Loss: 1.9033\n",
            "Epoch: 1/1... Step: 283... Loss: 1.6530... Val Loss: 1.8452\n",
            "Epoch: 1/1... Step: 284... Loss: 1.6038... Val Loss: 1.8178\n",
            "Epoch: 1/1... Step: 285... Loss: 1.7376... Val Loss: 1.7773\n",
            "Epoch: 1/1... Step: 286... Loss: 1.8126... Val Loss: 1.7914\n",
            "Epoch: 1/1... Step: 287... Loss: 1.6454... Val Loss: 1.7769\n",
            "Epoch: 1/1... Step: 288... Loss: 1.7324... Val Loss: 1.8324\n",
            "Epoch: 1/1... Step: 289... Loss: 1.6908... Val Loss: 1.7730\n",
            "Epoch: 1/1... Step: 290... Loss: 1.8127... Val Loss: 1.7987\n",
            "Epoch: 1/1... Step: 291... Loss: 1.4978... Val Loss: 1.8114\n",
            "Epoch: 1/1... Step: 292... Loss: 1.6121... Val Loss: 1.8236\n",
            "Epoch: 1/1... Step: 293... Loss: 1.7165... Val Loss: 1.7819\n",
            "Epoch: 1/1... Step: 294... Loss: 1.7101... Val Loss: 1.8085\n",
            "Epoch: 1/1... Step: 295... Loss: 1.6268... Val Loss: 1.7773\n",
            "Epoch: 1/1... Step: 296... Loss: 1.8322... Val Loss: 1.8014\n",
            "Epoch: 1/1... Step: 297... Loss: 1.6003... Val Loss: 1.7527\n",
            "Epoch: 1/1... Step: 298... Loss: 1.5955... Val Loss: 1.7832\n",
            "Epoch: 1/1... Step: 299... Loss: 1.6672... Val Loss: 1.7749\n",
            "Epoch: 1/1... Step: 300... Loss: 1.5251... Val Loss: 1.7862\n",
            "Epoch: 1/1... Step: 301... Loss: 1.7607... Val Loss: 1.7718\n",
            "Epoch: 1/1... Step: 302... Loss: 1.6612... Val Loss: 1.7855\n",
            "Epoch: 1/1... Step: 303... Loss: 1.5580... Val Loss: 1.7980\n",
            "Epoch: 1/1... Step: 304... Loss: 1.7644... Val Loss: 1.7646\n",
            "Epoch: 1/1... Step: 305... Loss: 1.7779... Val Loss: 1.7684\n",
            "Epoch: 1/1... Step: 306... Loss: 1.6991... Val Loss: 1.8094\n",
            "Epoch: 1/1... Step: 307... Loss: 1.6825... Val Loss: 1.7940\n",
            "Epoch: 1/1... Step: 308... Loss: 1.5472... Val Loss: 1.8256\n",
            "Epoch: 1/1... Step: 309... Loss: 1.6027... Val Loss: 1.7805\n",
            "Epoch: 1/1... Step: 310... Loss: 1.6673... Val Loss: 1.7586\n",
            "Epoch: 1/1... Step: 311... Loss: 1.5824... Val Loss: 1.7778\n",
            "Epoch: 1/1... Step: 312... Loss: 1.6435... Val Loss: 1.7452\n",
            "Epoch: 1/1... Step: 313... Loss: 1.8335... Val Loss: 1.7751\n",
            "Epoch: 1/1... Step: 314... Loss: 1.6286... Val Loss: 1.7831\n",
            "Epoch: 1/1... Step: 315... Loss: 1.4732... Val Loss: 1.8157\n",
            "Epoch: 1/1... Step: 316... Loss: 1.5280... Val Loss: 1.8174\n",
            "Epoch: 1/1... Step: 317... Loss: 1.5865... Val Loss: 1.7648\n",
            "Epoch: 1/1... Step: 318... Loss: 1.6810... Val Loss: 1.8083\n",
            "Epoch: 1/1... Step: 319... Loss: 1.6070... Val Loss: 1.7310\n",
            "Epoch: 1/1... Step: 320... Loss: 1.8162... Val Loss: 1.7654\n",
            "Epoch: 1/1... Step: 321... Loss: 1.5558... Val Loss: 1.7821\n",
            "Epoch: 1/1... Step: 322... Loss: 1.6329... Val Loss: 1.7600\n",
            "Epoch: 1/1... Step: 323... Loss: 1.6830... Val Loss: 1.7494\n",
            "Epoch: 1/1... Step: 324... Loss: 1.6328... Val Loss: 1.7547\n",
            "Epoch: 1/1... Step: 325... Loss: 1.5518... Val Loss: 1.7817\n",
            "Epoch: 1/1... Step: 326... Loss: 1.6991... Val Loss: 1.7878\n",
            "Epoch: 1/1... Step: 327... Loss: 1.5493... Val Loss: 1.7519\n",
            "Epoch: 1/1... Step: 328... Loss: 1.7188... Val Loss: 1.7523\n",
            "Epoch: 1/1... Step: 329... Loss: 1.5467... Val Loss: 1.7271\n",
            "Epoch: 1/1... Step: 330... Loss: 1.8042... Val Loss: 1.6923\n",
            "Epoch: 1/1... Step: 331... Loss: 1.7170... Val Loss: 1.7448\n",
            "Epoch: 1/1... Step: 332... Loss: 1.6343... Val Loss: 1.7460\n",
            "Epoch: 1/1... Step: 333... Loss: 1.4602... Val Loss: 1.7626\n",
            "Epoch: 1/1... Step: 334... Loss: 1.6738... Val Loss: 1.6953\n",
            "Epoch: 1/1... Step: 335... Loss: 1.6836... Val Loss: 1.7084\n",
            "Epoch: 1/1... Step: 336... Loss: 1.7980... Val Loss: 1.7309\n",
            "Epoch: 1/1... Step: 337... Loss: 1.6276... Val Loss: 1.7203\n",
            "Epoch: 1/1... Step: 338... Loss: 1.5676... Val Loss: 1.7227\n",
            "Epoch: 1/1... Step: 339... Loss: 1.5507... Val Loss: 1.7237\n",
            "Epoch: 1/1... Step: 340... Loss: 1.6992... Val Loss: 1.7160\n",
            "Epoch: 1/1... Step: 341... Loss: 1.5144... Val Loss: 1.7332\n",
            "Epoch: 1/1... Step: 342... Loss: 1.4703... Val Loss: 1.7173\n",
            "Epoch: 1/1... Step: 343... Loss: 1.6706... Val Loss: 1.7208\n",
            "Epoch: 1/1... Step: 344... Loss: 1.7035... Val Loss: 1.7063\n",
            "Epoch: 1/1... Step: 345... Loss: 1.4615... Val Loss: 1.7346\n",
            "Epoch: 1/1... Step: 346... Loss: 1.5251... Val Loss: 1.7328\n",
            "Epoch: 1/1... Step: 347... Loss: 1.5593... Val Loss: 1.7338\n",
            "Epoch: 1/1... Step: 348... Loss: 1.5770... Val Loss: 1.7550\n",
            "Epoch: 1/1... Step: 349... Loss: 1.5038... Val Loss: 1.7304\n",
            "Epoch: 1/1... Step: 350... Loss: 1.5683... Val Loss: 1.7159\n",
            "Epoch: 1/1... Step: 351... Loss: 1.5343... Val Loss: 1.7340\n",
            "Epoch: 1/1... Step: 352... Loss: 1.7390... Val Loss: 1.7245\n",
            "Epoch: 1/1... Step: 353... Loss: 1.5322... Val Loss: 1.7272\n",
            "Epoch: 1/1... Step: 354... Loss: 1.4436... Val Loss: 1.7056\n",
            "Epoch: 1/1... Step: 355... Loss: 1.5339... Val Loss: 1.7248\n",
            "Epoch: 1/1... Step: 356... Loss: 1.6017... Val Loss: 1.6899\n",
            "Epoch: 1/1... Step: 357... Loss: 1.4026... Val Loss: 1.7757\n",
            "Epoch: 1/1... Step: 358... Loss: 1.4590... Val Loss: 1.6825\n",
            "Epoch: 1/1... Step: 359... Loss: 1.7509... Val Loss: 1.7390\n",
            "Epoch: 1/1... Step: 360... Loss: 1.5338... Val Loss: 1.7169\n",
            "Epoch: 1/1... Step: 361... Loss: 1.8152... Val Loss: 1.7193\n",
            "Epoch: 1/1... Step: 362... Loss: 1.6452... Val Loss: 1.7665\n",
            "Epoch: 1/1... Step: 363... Loss: 1.3995... Val Loss: 1.7434\n",
            "Epoch: 1/1... Step: 364... Loss: 1.5710... Val Loss: 1.6975\n",
            "Epoch: 1/1... Step: 365... Loss: 1.6356... Val Loss: 1.7426\n",
            "Epoch: 1/1... Step: 366... Loss: 1.5944... Val Loss: 1.6872\n",
            "Epoch: 1/1... Step: 367... Loss: 1.6286... Val Loss: 1.7471\n",
            "Epoch: 1/1... Step: 368... Loss: 1.6660... Val Loss: 1.6769\n",
            "Epoch: 1/1... Step: 369... Loss: 1.7124... Val Loss: 1.7097\n",
            "Epoch: 1/1... Step: 370... Loss: 1.4968... Val Loss: 1.6902\n",
            "Epoch: 1/1... Step: 371... Loss: 1.5739... Val Loss: 1.7110\n",
            "Epoch: 1/1... Step: 372... Loss: 1.4982... Val Loss: 1.7191\n",
            "Epoch: 1/1... Step: 373... Loss: 1.5495... Val Loss: 1.7349\n",
            "Epoch: 1/1... Step: 374... Loss: 1.4025... Val Loss: 1.7234\n",
            "Epoch: 1/1... Step: 375... Loss: 1.7719... Val Loss: 1.7326\n",
            "Epoch: 1/1... Step: 376... Loss: 1.4771... Val Loss: 1.7302\n",
            "Epoch: 1/1... Step: 377... Loss: 1.6470... Val Loss: 1.7351\n",
            "Epoch: 1/1... Step: 378... Loss: 1.4936... Val Loss: 1.7134\n",
            "Epoch: 1/1... Step: 379... Loss: 1.5327... Val Loss: 1.6940\n",
            "Epoch: 1/1... Step: 380... Loss: 1.5579... Val Loss: 1.6777\n",
            "Epoch: 1/1... Step: 381... Loss: 1.6671... Val Loss: 1.7049\n",
            "Epoch: 1/1... Step: 382... Loss: 1.5934... Val Loss: 1.6934\n",
            "Epoch: 1/1... Step: 383... Loss: 1.6410... Val Loss: 1.7208\n",
            "Epoch: 1/1... Step: 384... Loss: 1.4306... Val Loss: 1.6920\n",
            "Epoch: 1/1... Step: 385... Loss: 1.4466... Val Loss: 1.6787\n",
            "Epoch: 1/1... Step: 386... Loss: 1.5425... Val Loss: 1.6752\n",
            "Epoch: 1/1... Step: 387... Loss: 1.6225... Val Loss: 1.6945\n",
            "Epoch: 1/1... Step: 388... Loss: 1.5344... Val Loss: 1.7270\n",
            "Epoch: 1/1... Step: 389... Loss: 1.6066... Val Loss: 1.6948\n",
            "Epoch: 1/1... Step: 390... Loss: 1.5858... Val Loss: 1.7148\n",
            "Epoch: 1/1... Step: 391... Loss: 1.5007... Val Loss: 1.6908\n",
            "Epoch: 1/1... Step: 392... Loss: 1.3500... Val Loss: 1.7432\n",
            "Epoch: 1/1... Step: 393... Loss: 1.5684... Val Loss: 1.6912\n",
            "Epoch: 1/1... Step: 394... Loss: 1.5278... Val Loss: 1.7123\n",
            "Epoch: 1/1... Step: 395... Loss: 1.4661... Val Loss: 1.6703\n",
            "Epoch: 1/1... Step: 396... Loss: 1.3390... Val Loss: 1.6947\n",
            "Epoch: 1/1... Step: 397... Loss: 1.4959... Val Loss: 1.6935\n",
            "Epoch: 1/1... Step: 398... Loss: 1.5649... Val Loss: 1.7034\n",
            "Epoch: 1/1... Step: 399... Loss: 1.6503... Val Loss: 1.6861\n",
            "Epoch: 1/1... Step: 400... Loss: 1.4562... Val Loss: 1.6958\n",
            "Epoch: 1/1... Step: 401... Loss: 1.5517... Val Loss: 1.6596\n",
            "Epoch: 1/1... Step: 402... Loss: 1.5201... Val Loss: 1.7319\n",
            "Epoch: 1/1... Step: 403... Loss: 1.4076... Val Loss: 1.6700\n",
            "Epoch: 1/1... Step: 404... Loss: 1.4734... Val Loss: 1.6935\n",
            "Epoch: 1/1... Step: 405... Loss: 1.6302... Val Loss: 1.6753\n",
            "Epoch: 1/1... Step: 406... Loss: 1.6386... Val Loss: 1.6816\n",
            "Epoch: 1/1... Step: 407... Loss: 1.5269... Val Loss: 1.6799\n",
            "Epoch: 1/1... Step: 408... Loss: 1.4832... Val Loss: 1.7296\n",
            "Epoch: 1/1... Step: 409... Loss: 1.6035... Val Loss: 1.6894\n",
            "Epoch: 1/1... Step: 410... Loss: 1.5074... Val Loss: 1.7306\n",
            "Epoch: 1/1... Step: 411... Loss: 1.4173... Val Loss: 1.6964\n",
            "Epoch: 1/1... Step: 412... Loss: 1.6368... Val Loss: 1.7058\n",
            "Epoch: 1/1... Step: 413... Loss: 1.3991... Val Loss: 1.7073\n",
            "Epoch: 1/1... Step: 414... Loss: 1.5811... Val Loss: 1.6779\n",
            "Epoch: 1/1... Step: 415... Loss: 1.4105... Val Loss: 1.6666\n",
            "Epoch: 1/1... Step: 416... Loss: 1.6584... Val Loss: 1.6782\n",
            "Epoch: 1/1... Step: 417... Loss: 1.5779... Val Loss: 1.6788\n",
            "Epoch: 1/1... Step: 418... Loss: 1.5343... Val Loss: 1.7416\n",
            "Epoch: 1/1... Step: 419... Loss: 1.5050... Val Loss: 1.6955\n",
            "Epoch: 1/1... Step: 420... Loss: 1.5987... Val Loss: 1.6901\n",
            "Epoch: 1/1... Step: 421... Loss: 1.5439... Val Loss: 1.6321\n",
            "Epoch: 1/1... Step: 422... Loss: 1.4742... Val Loss: 1.7239\n",
            "Epoch: 1/1... Step: 423... Loss: 1.3279... Val Loss: 1.6719\n",
            "Epoch: 1/1... Step: 424... Loss: 1.5425... Val Loss: 1.6921\n",
            "Epoch: 1/1... Step: 425... Loss: 1.4144... Val Loss: 1.6966\n",
            "Epoch: 1/1... Step: 426... Loss: 1.5541... Val Loss: 1.6811\n",
            "Epoch: 1/1... Step: 427... Loss: 1.4375... Val Loss: 1.7365\n",
            "Epoch: 1/1... Step: 428... Loss: 1.4331... Val Loss: 1.7182\n",
            "Epoch: 1/1... Step: 429... Loss: 1.2710... Val Loss: 1.6807\n",
            "Epoch: 1/1... Step: 430... Loss: 1.5004... Val Loss: 1.6537\n",
            "Epoch: 1/1... Step: 431... Loss: 1.4486... Val Loss: 1.6886\n",
            "Epoch: 1/1... Step: 432... Loss: 1.4894... Val Loss: 1.7112\n",
            "Epoch: 1/1... Step: 433... Loss: 1.5629... Val Loss: 1.6857\n",
            "Epoch: 1/1... Step: 434... Loss: 1.6518... Val Loss: 1.6665\n",
            "Epoch: 1/1... Step: 435... Loss: 1.4133... Val Loss: 1.6326\n",
            "Epoch: 1/1... Step: 436... Loss: 1.6419... Val Loss: 1.6107\n",
            "Epoch: 1/1... Step: 437... Loss: 1.4855... Val Loss: 1.6521\n",
            "Epoch: 1/1... Step: 438... Loss: 1.4083... Val Loss: 1.6524\n",
            "Epoch: 1/1... Step: 439... Loss: 1.3520... Val Loss: 1.6924\n",
            "Epoch: 1/1... Step: 440... Loss: 1.5506... Val Loss: 1.6312\n",
            "Epoch: 1/1... Step: 441... Loss: 1.5071... Val Loss: 1.7084\n",
            "Epoch: 1/1... Step: 442... Loss: 1.5014... Val Loss: 1.6424\n",
            "Epoch: 1/1... Step: 443... Loss: 1.4048... Val Loss: 1.6433\n",
            "Epoch: 1/1... Step: 444... Loss: 1.5773... Val Loss: 1.6346\n",
            "Epoch: 1/1... Step: 445... Loss: 1.5017... Val Loss: 1.6375\n",
            "Epoch: 1/1... Step: 446... Loss: 1.6120... Val Loss: 1.6412\n",
            "Epoch: 1/1... Step: 447... Loss: 1.5036... Val Loss: 1.6986\n",
            "Epoch: 1/1... Step: 448... Loss: 1.6852... Val Loss: 1.5940\n",
            "Epoch: 1/1... Step: 449... Loss: 1.5076... Val Loss: 1.6954\n",
            "Epoch: 1/1... Step: 450... Loss: 1.4720... Val Loss: 1.6381\n",
            "Epoch: 1/1... Step: 451... Loss: 1.6383... Val Loss: 1.6690\n",
            "Epoch: 1/1... Step: 452... Loss: 1.4655... Val Loss: 1.6385\n",
            "Epoch: 1/1... Step: 453... Loss: 1.5611... Val Loss: 1.7001\n",
            "Epoch: 1/1... Step: 454... Loss: 1.4533... Val Loss: 1.6342\n",
            "Epoch: 1/1... Step: 455... Loss: 1.5491... Val Loss: 1.6572\n",
            "Epoch: 1/1... Step: 456... Loss: 1.5693... Val Loss: 1.6166\n",
            "Epoch: 1/1... Step: 457... Loss: 1.3959... Val Loss: 1.6785\n",
            "Epoch: 1/1... Step: 458... Loss: 1.3862... Val Loss: 1.5939\n",
            "Epoch: 1/1... Step: 459... Loss: 1.5065... Val Loss: 1.7077\n",
            "Epoch: 1/1... Step: 460... Loss: 1.4169... Val Loss: 1.5966\n",
            "Epoch: 1/1... Step: 461... Loss: 1.6605... Val Loss: 1.6867\n",
            "Epoch: 1/1... Step: 462... Loss: 1.3437... Val Loss: 1.6365\n",
            "Epoch: 1/1... Step: 463... Loss: 1.7006... Val Loss: 1.6557\n",
            "Epoch: 1/1... Step: 464... Loss: 1.5147... Val Loss: 1.6040\n",
            "Epoch: 1/1... Step: 465... Loss: 1.7114... Val Loss: 1.7172\n",
            "Epoch: 1/1... Step: 466... Loss: 1.3582... Val Loss: 1.6557\n",
            "Epoch: 1/1... Step: 467... Loss: 1.6119... Val Loss: 1.6431\n",
            "Epoch: 1/1... Step: 468... Loss: 1.4247... Val Loss: 1.6357\n",
            "Epoch: 1/1... Step: 469... Loss: 1.4672... Val Loss: 1.6459\n",
            "Epoch: 1/1... Step: 470... Loss: 1.1799... Val Loss: 1.6355\n",
            "Epoch: 1/1... Step: 471... Loss: 1.6032... Val Loss: 1.6325\n",
            "Epoch: 1/1... Step: 472... Loss: 1.4737... Val Loss: 1.5948\n",
            "Epoch: 1/1... Step: 473... Loss: 1.8146... Val Loss: 1.6273\n",
            "Epoch: 1/1... Step: 474... Loss: 1.3495... Val Loss: 1.6506\n",
            "Epoch: 1/1... Step: 475... Loss: 1.5279... Val Loss: 1.6081\n",
            "Epoch: 1/1... Step: 476... Loss: 1.4708... Val Loss: 1.6324\n",
            "Epoch: 1/1... Step: 477... Loss: 1.6165... Val Loss: 1.6423\n",
            "Epoch: 1/1... Step: 478... Loss: 1.3980... Val Loss: 1.6284\n",
            "Epoch: 1/1... Step: 479... Loss: 1.4997... Val Loss: 1.6490\n",
            "Epoch: 1/1... Step: 480... Loss: 1.5495... Val Loss: 1.6219\n",
            "Epoch: 1/1... Step: 481... Loss: 1.6503... Val Loss: 1.6697\n",
            "Epoch: 1/1... Step: 482... Loss: 1.2678... Val Loss: 1.6319\n",
            "Epoch: 1/1... Step: 483... Loss: 1.6676... Val Loss: 1.6064\n",
            "Epoch: 1/1... Step: 484... Loss: 1.5471... Val Loss: 1.6138\n",
            "Epoch: 1/1... Step: 485... Loss: 1.5859... Val Loss: 1.6062\n",
            "Epoch: 1/1... Step: 486... Loss: 1.3495... Val Loss: 1.5956\n",
            "Epoch: 1/1... Step: 487... Loss: 1.5454... Val Loss: 1.5607\n",
            "Epoch: 1/1... Step: 488... Loss: 1.4347... Val Loss: 1.6607\n",
            "Epoch: 1/1... Step: 489... Loss: 1.4910... Val Loss: 1.6154\n",
            "Epoch: 1/1... Step: 490... Loss: 1.2403... Val Loss: 1.6031\n",
            "Epoch: 1/1... Step: 491... Loss: 1.5044... Val Loss: 1.6064\n",
            "Epoch: 1/1... Step: 492... Loss: 1.4942... Val Loss: 1.6821\n",
            "Epoch: 1/1... Step: 493... Loss: 1.4079... Val Loss: 1.6304\n",
            "Epoch: 1/1... Step: 494... Loss: 1.3495... Val Loss: 1.6642\n",
            "Epoch: 1/1... Step: 495... Loss: 1.4530... Val Loss: 1.6332\n",
            "Epoch: 1/1... Step: 496... Loss: 1.4187... Val Loss: 1.6782\n",
            "Epoch: 1/1... Step: 497... Loss: 1.4580... Val Loss: 1.6074\n",
            "Epoch: 1/1... Step: 498... Loss: 1.4611... Val Loss: 1.6735\n",
            "Epoch: 1/1... Step: 499... Loss: 1.4540... Val Loss: 1.6050\n",
            "Epoch: 1/1... Step: 500... Loss: 1.3927... Val Loss: 1.6777\n",
            "Epoch: 1/1... Step: 501... Loss: 1.6483... Val Loss: 1.5737\n",
            "Epoch: 1/1... Step: 502... Loss: 1.5720... Val Loss: 1.6789\n",
            "Epoch: 1/1... Step: 503... Loss: 1.4601... Val Loss: 1.5814\n",
            "Epoch: 1/1... Step: 504... Loss: 1.5171... Val Loss: 1.6477\n",
            "Epoch: 1/1... Step: 505... Loss: 1.3532... Val Loss: 1.6052\n",
            "Epoch: 1/1... Step: 506... Loss: 1.4857... Val Loss: 1.6589\n",
            "Epoch: 1/1... Step: 507... Loss: 1.3175... Val Loss: 1.6174\n",
            "Epoch: 1/1... Step: 508... Loss: 1.4451... Val Loss: 1.6092\n",
            "Epoch: 1/1... Step: 509... Loss: 1.4310... Val Loss: 1.5993\n",
            "Epoch: 1/1... Step: 510... Loss: 1.4729... Val Loss: 1.6456\n",
            "Epoch: 1/1... Step: 511... Loss: 1.4329... Val Loss: 1.5932\n",
            "Epoch: 1/1... Step: 512... Loss: 1.4579... Val Loss: 1.6555\n",
            "Epoch: 1/1... Step: 513... Loss: 1.4705... Val Loss: 1.5820\n",
            "Epoch: 1/1... Step: 514... Loss: 1.4972... Val Loss: 1.6781\n",
            "Epoch: 1/1... Step: 515... Loss: 1.3410... Val Loss: 1.5847\n",
            "Epoch: 1/1... Step: 516... Loss: 1.6401... Val Loss: 1.6497\n",
            "Epoch: 1/1... Step: 517... Loss: 1.3914... Val Loss: 1.5792\n",
            "Epoch: 1/1... Step: 518... Loss: 1.5948... Val Loss: 1.5973\n",
            "Epoch: 1/1... Step: 519... Loss: 1.2784... Val Loss: 1.5880\n",
            "Epoch: 1/1... Step: 520... Loss: 1.5827... Val Loss: 1.5961\n",
            "Epoch: 1/1... Step: 521... Loss: 1.3570... Val Loss: 1.5874\n",
            "Epoch: 1/1... Step: 522... Loss: 1.4907... Val Loss: 1.5937\n",
            "Epoch: 1/1... Step: 523... Loss: 1.2263... Val Loss: 1.6089\n",
            "Epoch: 1/1... Step: 524... Loss: 1.4650... Val Loss: 1.5963\n",
            "Epoch: 1/1... Step: 525... Loss: 1.3928... Val Loss: 1.5938\n",
            "Epoch: 1/1... Step: 526... Loss: 1.4168... Val Loss: 1.5811\n",
            "Epoch: 1/1... Step: 527... Loss: 1.4026... Val Loss: 1.5689\n",
            "Epoch: 1/1... Step: 528... Loss: 1.6333... Val Loss: 1.5725\n",
            "Epoch: 1/1... Step: 529... Loss: 1.3307... Val Loss: 1.6096\n",
            "Epoch: 1/1... Step: 530... Loss: 1.5677... Val Loss: 1.6110\n",
            "Epoch: 1/1... Step: 531... Loss: 1.3339... Val Loss: 1.5958\n",
            "Epoch: 1/1... Step: 532... Loss: 1.6328... Val Loss: 1.5713\n",
            "Epoch: 1/1... Step: 533... Loss: 1.4231... Val Loss: 1.6095\n",
            "Epoch: 1/1... Step: 534... Loss: 1.5193... Val Loss: 1.5489\n",
            "Epoch: 1/1... Step: 535... Loss: 1.3799... Val Loss: 1.5757\n",
            "Epoch: 1/1... Step: 536... Loss: 1.5745... Val Loss: 1.5536\n",
            "Epoch: 1/1... Step: 537... Loss: 1.2731... Val Loss: 1.5872\n",
            "Epoch: 1/1... Step: 538... Loss: 1.4999... Val Loss: 1.5656\n",
            "Epoch: 1/1... Step: 539... Loss: 1.4769... Val Loss: 1.5718\n",
            "Epoch: 1/1... Step: 540... Loss: 1.3118... Val Loss: 1.5801\n",
            "Epoch: 1/1... Step: 541... Loss: 1.3777... Val Loss: 1.5743\n",
            "Epoch: 1/1... Step: 542... Loss: 1.3992... Val Loss: 1.5510\n",
            "Epoch: 1/1... Step: 543... Loss: 1.4141... Val Loss: 1.5828\n",
            "Epoch: 1/1... Step: 544... Loss: 1.3462... Val Loss: 1.5970\n",
            "Epoch: 1/1... Step: 545... Loss: 1.2921... Val Loss: 1.5820\n",
            "Epoch: 1/1... Step: 546... Loss: 1.3990... Val Loss: 1.5599\n",
            "Epoch: 1/1... Step: 547... Loss: 1.4690... Val Loss: 1.6051\n",
            "Epoch: 1/1... Step: 548... Loss: 1.3760... Val Loss: 1.5960\n",
            "Epoch: 1/1... Step: 549... Loss: 1.4182... Val Loss: 1.5994\n",
            "Epoch: 1/1... Step: 550... Loss: 1.4688... Val Loss: 1.5942\n",
            "Epoch: 1/1... Step: 551... Loss: 1.4141... Val Loss: 1.6036\n",
            "Epoch: 1/1... Step: 552... Loss: 1.5192... Val Loss: 1.5838\n",
            "Epoch: 1/1... Step: 553... Loss: 1.4216... Val Loss: 1.6045\n",
            "Epoch: 1/1... Step: 554... Loss: 1.4999... Val Loss: 1.5853\n",
            "Epoch: 1/1... Step: 555... Loss: 1.3449... Val Loss: 1.6164\n",
            "Epoch: 1/1... Step: 556... Loss: 1.4293... Val Loss: 1.6368\n",
            "Epoch: 1/1... Step: 557... Loss: 1.3093... Val Loss: 1.6301\n",
            "Epoch: 1/1... Step: 558... Loss: 1.4125... Val Loss: 1.6007\n",
            "Epoch: 1/1... Step: 559... Loss: 1.5034... Val Loss: 1.5981\n",
            "Epoch: 1/1... Step: 560... Loss: 1.4125... Val Loss: 1.6039\n",
            "Epoch: 1/1... Step: 561... Loss: 1.3989... Val Loss: 1.6159\n",
            "Epoch: 1/1... Step: 562... Loss: 1.4089... Val Loss: 1.6105\n",
            "Epoch: 1/1... Step: 563... Loss: 1.5562... Val Loss: 1.5876\n",
            "Epoch: 1/1... Step: 564... Loss: 1.5507... Val Loss: 1.6006\n",
            "Epoch: 1/1... Step: 565... Loss: 1.4384... Val Loss: 1.5726\n",
            "Epoch: 1/1... Step: 566... Loss: 1.4252... Val Loss: 1.5702\n",
            "Epoch: 1/1... Step: 567... Loss: 1.5578... Val Loss: 1.5541\n",
            "Epoch: 1/1... Step: 568... Loss: 1.5445... Val Loss: 1.5506\n",
            "Epoch: 1/1... Step: 569... Loss: 1.5870... Val Loss: 1.5734\n",
            "Epoch: 1/1... Step: 570... Loss: 1.3608... Val Loss: 1.5838\n",
            "Epoch: 1/1... Step: 571... Loss: 1.4331... Val Loss: 1.6023\n",
            "Epoch: 1/1... Step: 572... Loss: 1.3886... Val Loss: 1.5818\n",
            "Epoch: 1/1... Step: 573... Loss: 1.4942... Val Loss: 1.5798\n",
            "Epoch: 1/1... Step: 574... Loss: 1.2065... Val Loss: 1.5565\n",
            "Epoch: 1/1... Step: 575... Loss: 1.4520... Val Loss: 1.5889\n",
            "Epoch: 1/1... Step: 576... Loss: 1.2934... Val Loss: 1.6054\n",
            "Epoch: 1/1... Step: 577... Loss: 1.4398... Val Loss: 1.5884\n",
            "Epoch: 1/1... Step: 578... Loss: 1.4354... Val Loss: 1.5613\n",
            "Epoch: 1/1... Step: 579... Loss: 1.5335... Val Loss: 1.6146\n",
            "Epoch: 1/1... Step: 580... Loss: 1.1965... Val Loss: 1.5646\n",
            "Epoch: 1/1... Step: 581... Loss: 1.4498... Val Loss: 1.6369\n",
            "Epoch: 1/1... Step: 582... Loss: 1.4068... Val Loss: 1.5620\n",
            "Epoch: 1/1... Step: 583... Loss: 1.5844... Val Loss: 1.6093\n",
            "Epoch: 1/1... Step: 584... Loss: 1.2975... Val Loss: 1.5298\n",
            "Epoch: 1/1... Step: 585... Loss: 1.6223... Val Loss: 1.5934\n",
            "Epoch: 1/1... Step: 586... Loss: 1.3262... Val Loss: 1.5603\n",
            "Epoch: 1/1... Step: 587... Loss: 1.5716... Val Loss: 1.5630\n",
            "Epoch: 1/1... Step: 588... Loss: 1.2565... Val Loss: 1.5349\n",
            "Epoch: 1/1... Step: 589... Loss: 1.5714... Val Loss: 1.5431\n",
            "Epoch: 1/1... Step: 590... Loss: 1.3415... Val Loss: 1.5508\n",
            "Epoch: 1/1... Step: 591... Loss: 1.5532... Val Loss: 1.5981\n",
            "Epoch: 1/1... Step: 592... Loss: 1.3015... Val Loss: 1.5943\n",
            "Epoch: 1/1... Step: 593... Loss: 1.5044... Val Loss: 1.6127\n",
            "Epoch: 1/1... Step: 594... Loss: 1.2941... Val Loss: 1.5970\n",
            "Epoch: 1/1... Step: 595... Loss: 1.2967... Val Loss: 1.5938\n",
            "Epoch: 1/1... Step: 596... Loss: 1.2294... Val Loss: 1.5472\n",
            "Epoch: 1/1... Step: 597... Loss: 1.4625... Val Loss: 1.5714\n",
            "Epoch: 1/1... Step: 598... Loss: 1.2942... Val Loss: 1.5380\n",
            "Epoch: 1/1... Step: 599... Loss: 1.5287... Val Loss: 1.5763\n",
            "Epoch: 1/1... Step: 600... Loss: 1.3899... Val Loss: 1.5617\n",
            "Epoch: 1/1... Step: 601... Loss: 1.4622... Val Loss: 1.5877\n",
            "Epoch: 1/1... Step: 602... Loss: 1.1111... Val Loss: 1.6172\n",
            "Epoch: 1/1... Step: 603... Loss: 1.3336... Val Loss: 1.5863\n",
            "Epoch: 1/1... Step: 604... Loss: 1.4174... Val Loss: 1.5596\n",
            "Epoch: 1/1... Step: 605... Loss: 1.5933... Val Loss: 1.5575\n",
            "Epoch: 1/1... Step: 606... Loss: 1.6030... Val Loss: 1.5586\n",
            "Epoch: 1/1... Step: 607... Loss: 1.4393... Val Loss: 1.5496\n",
            "Epoch: 1/1... Step: 608... Loss: 1.3697... Val Loss: 1.5396\n",
            "Epoch: 1/1... Step: 609... Loss: 1.4468... Val Loss: 1.5416\n",
            "Epoch: 1/1... Step: 610... Loss: 1.3197... Val Loss: 1.5564\n",
            "Epoch: 1/1... Step: 611... Loss: 1.3721... Val Loss: 1.5348\n",
            "Epoch: 1/1... Step: 612... Loss: 1.4663... Val Loss: 1.5526\n",
            "Epoch: 1/1... Step: 613... Loss: 1.3063... Val Loss: 1.5519\n",
            "Epoch: 1/1... Step: 614... Loss: 1.2454... Val Loss: 1.5870\n",
            "Epoch: 1/1... Step: 615... Loss: 1.4523... Val Loss: 1.5691\n",
            "Epoch: 1/1... Step: 616... Loss: 1.5717... Val Loss: 1.5690\n",
            "Epoch: 1/1... Step: 617... Loss: 1.3113... Val Loss: 1.5630\n",
            "Epoch: 1/1... Step: 618... Loss: 1.4267... Val Loss: 1.6015\n",
            "Epoch: 1/1... Step: 619... Loss: 1.3643... Val Loss: 1.5516\n",
            "Epoch: 1/1... Step: 620... Loss: 1.4348... Val Loss: 1.5762\n",
            "Epoch: 1/1... Step: 621... Loss: 1.3737... Val Loss: 1.5536\n",
            "Epoch: 1/1... Step: 622... Loss: 1.3573... Val Loss: 1.5696\n",
            "Epoch: 1/1... Step: 623... Loss: 1.3923... Val Loss: 1.5242\n",
            "Epoch: 1/1... Step: 624... Loss: 1.5081... Val Loss: 1.5630\n",
            "Epoch: 1/1... Step: 625... Loss: 1.2114... Val Loss: 1.5738\n",
            "Epoch: 1/1... Step: 626... Loss: 1.3295... Val Loss: 1.5356\n",
            "Epoch: 1/1... Step: 627... Loss: 1.5037... Val Loss: 1.5601\n",
            "Epoch: 1/1... Step: 628... Loss: 1.3093... Val Loss: 1.5505\n",
            "Epoch: 1/1... Step: 629... Loss: 1.4509... Val Loss: 1.5438\n",
            "Epoch: 1/1... Step: 630... Loss: 1.4468... Val Loss: 1.5631\n",
            "Epoch: 1/1... Step: 631... Loss: 1.3632... Val Loss: 1.5193\n",
            "Epoch: 1/1... Step: 632... Loss: 1.5105... Val Loss: 1.5683\n",
            "Epoch: 1/1... Step: 633... Loss: 1.4203... Val Loss: 1.5168\n",
            "Epoch: 1/1... Step: 634... Loss: 1.5718... Val Loss: 1.5278\n",
            "Epoch: 1/1... Step: 635... Loss: 1.3917... Val Loss: 1.5393\n",
            "Epoch: 1/1... Step: 636... Loss: 1.3489... Val Loss: 1.5254\n",
            "Epoch: 1/1... Step: 637... Loss: 1.5437... Val Loss: 1.5217\n",
            "Epoch: 1/1... Step: 638... Loss: 1.4558... Val Loss: 1.5331\n",
            "Epoch: 1/1... Step: 639... Loss: 1.2471... Val Loss: 1.5213\n",
            "Epoch: 1/1... Step: 640... Loss: 1.3180... Val Loss: 1.5293\n",
            "Epoch: 1/1... Step: 641... Loss: 1.3348... Val Loss: 1.5325\n",
            "Epoch: 1/1... Step: 642... Loss: 1.2852... Val Loss: 1.5689\n",
            "Epoch: 1/1... Step: 643... Loss: 1.3348... Val Loss: 1.5468\n",
            "Epoch: 1/1... Step: 644... Loss: 1.2706... Val Loss: 1.5279\n",
            "Epoch: 1/1... Step: 645... Loss: 1.3628... Val Loss: 1.5626\n",
            "Epoch: 1/1... Step: 646... Loss: 1.3285... Val Loss: 1.5402\n",
            "Epoch: 1/1... Step: 647... Loss: 1.4381... Val Loss: 1.5298\n",
            "Epoch: 1/1... Step: 648... Loss: 1.3904... Val Loss: 1.5365\n",
            "Epoch: 1/1... Step: 649... Loss: 1.4908... Val Loss: 1.5516\n",
            "Epoch: 1/1... Step: 650... Loss: 1.2462... Val Loss: 1.5437\n",
            "Epoch: 1/1... Step: 651... Loss: 1.2812... Val Loss: 1.5586\n",
            "Epoch: 1/1... Step: 652... Loss: 1.3545... Val Loss: 1.5099\n",
            "Epoch: 1/1... Step: 653... Loss: 1.4090... Val Loss: 1.5584\n",
            "Epoch: 1/1... Step: 654... Loss: 1.3510... Val Loss: 1.5539\n",
            "Epoch: 1/1... Step: 655... Loss: 1.3921... Val Loss: 1.5658\n",
            "Epoch: 1/1... Step: 656... Loss: 1.3137... Val Loss: 1.5402\n",
            "Epoch: 1/1... Step: 657... Loss: 1.3786... Val Loss: 1.5436\n",
            "Epoch: 1/1... Step: 658... Loss: 1.4290... Val Loss: 1.5139\n",
            "Epoch: 1/1... Step: 659... Loss: 1.5085... Val Loss: 1.4991\n",
            "Epoch: 1/1... Step: 660... Loss: 1.3310... Val Loss: 1.5175\n",
            "Epoch: 1/1... Step: 661... Loss: 1.2572... Val Loss: 1.5189\n",
            "Epoch: 1/1... Step: 662... Loss: 1.3865... Val Loss: 1.5384\n",
            "Epoch: 1/1... Step: 663... Loss: 1.4056... Val Loss: 1.5328\n",
            "Epoch: 1/1... Step: 664... Loss: 1.2885... Val Loss: 1.5689\n",
            "Epoch: 1/1... Step: 665... Loss: 1.4358... Val Loss: 1.5426\n",
            "Epoch: 1/1... Step: 666... Loss: 1.3244... Val Loss: 1.5470\n",
            "Epoch: 1/1... Step: 667... Loss: 1.4026... Val Loss: 1.5457\n",
            "Epoch: 1/1... Step: 668... Loss: 1.3700... Val Loss: 1.5340\n",
            "Epoch: 1/1... Step: 669... Loss: 1.2692... Val Loss: 1.5184\n",
            "Epoch: 1/1... Step: 670... Loss: 1.3184... Val Loss: 1.5327\n",
            "Epoch: 1/1... Step: 671... Loss: 1.3475... Val Loss: 1.5255\n",
            "Epoch: 1/1... Step: 672... Loss: 1.3009... Val Loss: 1.5382\n",
            "Epoch: 1/1... Step: 673... Loss: 1.4025... Val Loss: 1.5065\n",
            "Epoch: 1/1... Step: 674... Loss: 1.2620... Val Loss: 1.5426\n",
            "Epoch: 1/1... Step: 675... Loss: 1.2876... Val Loss: 1.4848\n",
            "Epoch: 1/1... Step: 676... Loss: 1.3588... Val Loss: 1.5201\n",
            "Epoch: 1/1... Step: 677... Loss: 1.2984... Val Loss: 1.4818\n",
            "Epoch: 1/1... Step: 678... Loss: 1.4307... Val Loss: 1.5448\n",
            "Epoch: 1/1... Step: 679... Loss: 1.1889... Val Loss: 1.5048\n",
            "Epoch: 1/1... Step: 680... Loss: 1.3238... Val Loss: 1.5760\n",
            "Epoch: 1/1... Step: 681... Loss: 1.1871... Val Loss: 1.5343\n",
            "Epoch: 1/1... Step: 682... Loss: 1.5091... Val Loss: 1.5041\n",
            "Epoch: 1/1... Step: 683... Loss: 1.3465... Val Loss: 1.5385\n",
            "Epoch: 1/1... Step: 684... Loss: 1.4350... Val Loss: 1.5429\n",
            "Epoch: 1/1... Step: 685... Loss: 1.1835... Val Loss: 1.5214\n",
            "Epoch: 1/1... Step: 686... Loss: 1.2958... Val Loss: 1.5398\n",
            "Epoch: 1/1... Step: 687... Loss: 1.3430... Val Loss: 1.5050\n",
            "Epoch: 1/1... Step: 688... Loss: 1.5835... Val Loss: 1.4984\n",
            "Epoch: 1/1... Step: 689... Loss: 1.3588... Val Loss: 1.5526\n",
            "Epoch: 1/1... Step: 690... Loss: 1.2865... Val Loss: 1.5287\n",
            "Epoch: 1/1... Step: 691... Loss: 1.3742... Val Loss: 1.5042\n",
            "Epoch: 1/1... Step: 692... Loss: 1.3417... Val Loss: 1.5014\n",
            "Epoch: 1/1... Step: 693... Loss: 1.2594... Val Loss: 1.4986\n",
            "Epoch: 1/1... Step: 694... Loss: 1.3367... Val Loss: 1.4961\n",
            "Epoch: 1/1... Step: 695... Loss: 1.4000... Val Loss: 1.4811\n",
            "Epoch: 1/1... Step: 696... Loss: 1.4766... Val Loss: 1.4788\n",
            "Epoch: 1/1... Step: 697... Loss: 1.3114... Val Loss: 1.4975\n",
            "Epoch: 1/1... Step: 698... Loss: 1.3140... Val Loss: 1.4876\n",
            "Epoch: 1/1... Step: 699... Loss: 1.3437... Val Loss: 1.4960\n",
            "Epoch: 1/1... Step: 700... Loss: 1.4120... Val Loss: 1.5045\n",
            "Epoch: 1/1... Step: 701... Loss: 1.3846... Val Loss: 1.4881\n",
            "Epoch: 1/1... Step: 702... Loss: 1.4476... Val Loss: 1.4709\n",
            "Epoch: 1/1... Step: 703... Loss: 1.3690... Val Loss: 1.4953\n",
            "Epoch: 1/1... Step: 704... Loss: 1.2440... Val Loss: 1.4921\n",
            "Epoch: 1/1... Step: 705... Loss: 1.2709... Val Loss: 1.5202\n",
            "Epoch: 1/1... Step: 706... Loss: 1.2117... Val Loss: 1.5202\n",
            "Epoch: 1/1... Step: 707... Loss: 1.3507... Val Loss: 1.4830\n",
            "Epoch: 1/1... Step: 708... Loss: 1.4878... Val Loss: 1.4861\n",
            "Epoch: 1/1... Step: 709... Loss: 1.3491... Val Loss: 1.4973\n",
            "Epoch: 1/1... Step: 710... Loss: 1.3007... Val Loss: 1.4845\n",
            "Epoch: 1/1... Step: 711... Loss: 1.3002... Val Loss: 1.5223\n",
            "Epoch: 1/1... Step: 712... Loss: 1.4295... Val Loss: 1.4539\n",
            "Epoch: 1/1... Step: 713... Loss: 1.5461... Val Loss: 1.4802\n",
            "Epoch: 1/1... Step: 714... Loss: 1.3950... Val Loss: 1.4860\n",
            "Epoch: 1/1... Step: 715... Loss: 1.4269... Val Loss: 1.4819\n",
            "Epoch: 1/1... Step: 716... Loss: 1.5438... Val Loss: 1.4567\n",
            "Epoch: 1/1... Step: 717... Loss: 1.4806... Val Loss: 1.4801\n",
            "Epoch: 1/1... Step: 718... Loss: 1.2933... Val Loss: 1.5083\n",
            "Epoch: 1/1... Step: 719... Loss: 1.4013... Val Loss: 1.5163\n",
            "Epoch: 1/1... Step: 720... Loss: 1.2439... Val Loss: 1.5180\n",
            "Epoch: 1/1... Step: 721... Loss: 1.3667... Val Loss: 1.5325\n",
            "Epoch: 1/1... Step: 722... Loss: 1.1873... Val Loss: 1.5304\n",
            "Epoch: 1/1... Step: 723... Loss: 1.3772... Val Loss: 1.5153\n",
            "Epoch: 1/1... Step: 724... Loss: 1.3130... Val Loss: 1.5078\n",
            "Epoch: 1/1... Step: 725... Loss: 1.3293... Val Loss: 1.5283\n",
            "Epoch: 1/1... Step: 726... Loss: 1.2697... Val Loss: 1.5273\n",
            "Epoch: 1/1... Step: 727... Loss: 1.4654... Val Loss: 1.4847\n",
            "Epoch: 1/1... Step: 728... Loss: 1.5602... Val Loss: 1.4895\n",
            "Epoch: 1/1... Step: 729... Loss: 1.3633... Val Loss: 1.4943\n",
            "Epoch: 1/1... Step: 730... Loss: 1.2974... Val Loss: 1.4875\n",
            "Epoch: 1/1... Step: 731... Loss: 1.3972... Val Loss: 1.4845\n",
            "Epoch: 1/1... Step: 732... Loss: 1.2471... Val Loss: 1.5012\n",
            "Epoch: 1/1... Step: 733... Loss: 1.4984... Val Loss: 1.4784\n",
            "Epoch: 1/1... Step: 734... Loss: 1.3420... Val Loss: 1.5156\n",
            "Epoch: 1/1... Step: 735... Loss: 1.4211... Val Loss: 1.4907\n",
            "Epoch: 1/1... Step: 736... Loss: 1.4800... Val Loss: 1.5363\n",
            "Epoch: 1/1... Step: 737... Loss: 1.2989... Val Loss: 1.5036\n",
            "Epoch: 1/1... Step: 738... Loss: 1.2473... Val Loss: 1.5066\n",
            "Epoch: 1/1... Step: 739... Loss: 1.4840... Val Loss: 1.4534\n",
            "Epoch: 1/1... Step: 740... Loss: 1.3287... Val Loss: 1.5121\n",
            "Epoch: 1/1... Step: 741... Loss: 1.3597... Val Loss: 1.4658\n",
            "Epoch: 1/1... Step: 742... Loss: 1.2927... Val Loss: 1.5112\n",
            "Epoch: 1/1... Step: 743... Loss: 1.4244... Val Loss: 1.4502\n",
            "Epoch: 1/1... Step: 744... Loss: 1.3810... Val Loss: 1.5040\n",
            "Epoch: 1/1... Step: 745... Loss: 1.3486... Val Loss: 1.4435\n",
            "Epoch: 1/1... Step: 746... Loss: 1.3543... Val Loss: 1.4795\n",
            "Epoch: 1/1... Step: 747... Loss: 1.2610... Val Loss: 1.4539\n",
            "Epoch: 1/1... Step: 748... Loss: 1.2974... Val Loss: 1.5392\n",
            "Epoch: 1/1... Step: 749... Loss: 1.1066... Val Loss: 1.4444\n",
            "Epoch: 1/1... Step: 750... Loss: 1.4524... Val Loss: 1.5032\n",
            "Epoch: 1/1... Step: 751... Loss: 1.2097... Val Loss: 1.4689\n",
            "Epoch: 1/1... Step: 752... Loss: 1.3248... Val Loss: 1.5299\n",
            "Epoch: 1/1... Step: 753... Loss: 1.1422... Val Loss: 1.4910\n",
            "Epoch: 1/1... Step: 754... Loss: 1.4122... Val Loss: 1.5031\n",
            "Epoch: 1/1... Step: 755... Loss: 1.2942... Val Loss: 1.4977\n",
            "Epoch: 1/1... Step: 756... Loss: 1.4621... Val Loss: 1.5140\n",
            "Epoch: 1/1... Step: 757... Loss: 1.1974... Val Loss: 1.4965\n",
            "Epoch: 1/1... Step: 758... Loss: 1.4250... Val Loss: 1.4800\n",
            "Epoch: 1/1... Step: 759... Loss: 1.2582... Val Loss: 1.4793\n",
            "Epoch: 1/1... Step: 760... Loss: 1.3294... Val Loss: 1.4831\n",
            "Epoch: 1/1... Step: 761... Loss: 1.3364... Val Loss: 1.4794\n",
            "Epoch: 1/1... Step: 762... Loss: 1.3786... Val Loss: 1.4867\n",
            "Epoch: 1/1... Step: 763... Loss: 1.2665... Val Loss: 1.5050\n",
            "Epoch: 1/1... Step: 764... Loss: 1.2909... Val Loss: 1.4781\n",
            "Epoch: 1/1... Step: 765... Loss: 1.2828... Val Loss: 1.4993\n",
            "Epoch: 1/1... Step: 766... Loss: 1.2799... Val Loss: 1.4965\n",
            "Epoch: 1/1... Step: 767... Loss: 1.2979... Val Loss: 1.4889\n",
            "Epoch: 1/1... Step: 768... Loss: 1.3294... Val Loss: 1.4629\n",
            "Epoch: 1/1... Step: 769... Loss: 1.2265... Val Loss: 1.4800\n",
            "Epoch: 1/1... Step: 770... Loss: 1.3016... Val Loss: 1.4604\n",
            "Epoch: 1/1... Step: 771... Loss: 1.3888... Val Loss: 1.4811\n",
            "Epoch: 1/1... Step: 772... Loss: 1.4482... Val Loss: 1.4551\n",
            "Epoch: 1/1... Step: 773... Loss: 1.3688... Val Loss: 1.4810\n",
            "Epoch: 1/1... Step: 774... Loss: 1.2311... Val Loss: 1.4698\n",
            "Epoch: 1/1... Step: 775... Loss: 1.2823... Val Loss: 1.4560\n",
            "Epoch: 1/1... Step: 776... Loss: 1.3859... Val Loss: 1.4414\n",
            "Epoch: 1/1... Step: 777... Loss: 1.1800... Val Loss: 1.4841\n",
            "Epoch: 1/1... Step: 778... Loss: 1.3046... Val Loss: 1.4720\n",
            "Epoch: 1/1... Step: 779... Loss: 1.4370... Val Loss: 1.4691\n",
            "Epoch: 1/1... Step: 780... Loss: 1.2237... Val Loss: 1.4754\n",
            "Epoch: 1/1... Step: 781... Loss: 1.2511... Val Loss: 1.4802\n",
            "Epoch: 1/1... Step: 782... Loss: 1.2430... Val Loss: 1.4581\n",
            "Epoch: 1/1... Step: 783... Loss: 1.2721... Val Loss: 1.4467\n",
            "Epoch: 1/1... Step: 784... Loss: 1.1940... Val Loss: 1.4604\n",
            "Epoch: 1/1... Step: 785... Loss: 1.2631... Val Loss: 1.4499\n",
            "Epoch: 1/1... Step: 786... Loss: 1.3393... Val Loss: 1.4281\n",
            "Epoch: 1/1... Step: 787... Loss: 1.4580... Val Loss: 1.4572\n",
            "Epoch: 1/1... Step: 788... Loss: 1.2213... Val Loss: 1.4421\n",
            "Epoch: 1/1... Step: 789... Loss: 1.3316... Val Loss: 1.4370\n",
            "Epoch: 1/1... Step: 790... Loss: 1.2693... Val Loss: 1.4446\n",
            "Epoch: 1/1... Step: 791... Loss: 1.4056... Val Loss: 1.4552\n",
            "Epoch: 1/1... Step: 792... Loss: 1.3555... Val Loss: 1.4473\n",
            "Epoch: 1/1... Step: 793... Loss: 1.3656... Val Loss: 1.4507\n",
            "Epoch: 1/1... Step: 794... Loss: 1.3505... Val Loss: 1.4185\n",
            "Epoch: 1/1... Step: 795... Loss: 1.3071... Val Loss: 1.4451\n",
            "Epoch: 1/1... Step: 796... Loss: 1.3403... Val Loss: 1.4470\n",
            "Epoch: 1/1... Step: 797... Loss: 1.4575... Val Loss: 1.4577\n",
            "Epoch: 1/1... Step: 798... Loss: 1.3663... Val Loss: 1.4945\n",
            "Epoch: 1/1... Step: 799... Loss: 1.2401... Val Loss: 1.4723\n",
            "Epoch: 1/1... Step: 800... Loss: 1.3795... Val Loss: 1.4802\n",
            "Epoch: 1/1... Step: 801... Loss: 1.2451... Val Loss: 1.4551\n",
            "Epoch: 1/1... Step: 802... Loss: 1.2512... Val Loss: 1.4643\n",
            "Epoch: 1/1... Step: 803... Loss: 1.1998... Val Loss: 1.4287\n",
            "Epoch: 1/1... Step: 804... Loss: 1.3094... Val Loss: 1.4725\n",
            "Epoch: 1/1... Step: 805... Loss: 1.3552... Val Loss: 1.4400\n",
            "Epoch: 1/1... Step: 806... Loss: 1.4015... Val Loss: 1.4524\n",
            "Epoch: 1/1... Step: 807... Loss: 1.2663... Val Loss: 1.4558\n",
            "Epoch: 1/1... Step: 808... Loss: 1.3413... Val Loss: 1.4937\n",
            "Epoch: 1/1... Step: 809... Loss: 1.3694... Val Loss: 1.4576\n",
            "Epoch: 1/1... Step: 810... Loss: 1.3655... Val Loss: 1.4397\n",
            "Epoch: 1/1... Step: 811... Loss: 1.2205... Val Loss: 1.4513\n",
            "Epoch: 1/1... Step: 812... Loss: 1.1806... Val Loss: 1.4418\n",
            "Epoch: 1/1... Step: 813... Loss: 1.2754... Val Loss: 1.4548\n",
            "Epoch: 1/1... Step: 814... Loss: 1.3819... Val Loss: 1.4331\n",
            "Epoch: 1/1... Step: 815... Loss: 1.3473... Val Loss: 1.4288\n",
            "Epoch: 1/1... Step: 816... Loss: 1.3784... Val Loss: 1.4676\n",
            "Epoch: 1/1... Step: 817... Loss: 1.3548... Val Loss: 1.4638\n",
            "Epoch: 1/1... Step: 818... Loss: 1.2317... Val Loss: 1.4598\n",
            "Epoch: 1/1... Step: 819... Loss: 1.2189... Val Loss: 1.4676\n",
            "Epoch: 1/1... Step: 820... Loss: 1.3414... Val Loss: 1.4475\n",
            "Epoch: 1/1... Step: 821... Loss: 1.2725... Val Loss: 1.4604\n",
            "Epoch: 1/1... Step: 822... Loss: 1.3335... Val Loss: 1.4693\n",
            "Epoch: 1/1... Step: 823... Loss: 1.2739... Val Loss: 1.4431\n",
            "Epoch: 1/1... Step: 824... Loss: 1.3921... Val Loss: 1.4447\n",
            "Epoch: 1/1... Step: 825... Loss: 1.2378... Val Loss: 1.4576\n",
            "Epoch: 1/1... Step: 826... Loss: 1.2780... Val Loss: 1.4486\n",
            "Epoch: 1/1... Step: 827... Loss: 1.2727... Val Loss: 1.4574\n",
            "Epoch: 1/1... Step: 828... Loss: 1.2260... Val Loss: 1.4356\n",
            "Epoch: 1/1... Step: 829... Loss: 1.3967... Val Loss: 1.4196\n",
            "Epoch: 1/1... Step: 830... Loss: 1.3102... Val Loss: 1.4338\n",
            "Epoch: 1/1... Step: 831... Loss: 1.2398... Val Loss: 1.4430\n",
            "Epoch: 1/1... Step: 832... Loss: 1.2980... Val Loss: 1.4172\n",
            "Epoch: 1/1... Step: 833... Loss: 1.3351... Val Loss: 1.4279\n",
            "Epoch: 1/1... Step: 834... Loss: 1.2514... Val Loss: 1.4097\n",
            "Epoch: 1/1... Step: 835... Loss: 1.4058... Val Loss: 1.4130\n",
            "Epoch: 1/1... Step: 836... Loss: 1.3165... Val Loss: 1.4327\n",
            "Epoch: 1/1... Step: 837... Loss: 1.2414... Val Loss: 1.4312\n",
            "Epoch: 1/1... Step: 838... Loss: 1.2865... Val Loss: 1.4228\n",
            "Epoch: 1/1... Step: 839... Loss: 1.3323... Val Loss: 1.4415\n",
            "Epoch: 1/1... Step: 840... Loss: 1.2095... Val Loss: 1.4551\n",
            "Epoch: 1/1... Step: 841... Loss: 1.1530... Val Loss: 1.4378\n",
            "Epoch: 1/1... Step: 842... Loss: 1.1964... Val Loss: 1.4207\n",
            "Epoch: 1/1... Step: 843... Loss: 1.2360... Val Loss: 1.4188\n",
            "Epoch: 1/1... Step: 844... Loss: 1.3541... Val Loss: 1.4315\n",
            "Epoch: 1/1... Step: 845... Loss: 1.3390... Val Loss: 1.4319\n",
            "Epoch: 1/1... Step: 846... Loss: 1.3495... Val Loss: 1.4308\n",
            "Epoch: 1/1... Step: 847... Loss: 1.4318... Val Loss: 1.4493\n",
            "Epoch: 1/1... Step: 848... Loss: 1.2860... Val Loss: 1.4329\n",
            "Epoch: 1/1... Step: 849... Loss: 1.3736... Val Loss: 1.4550\n",
            "Epoch: 1/1... Step: 850... Loss: 1.2563... Val Loss: 1.4626\n",
            "Epoch: 1/1... Step: 851... Loss: 1.1721... Val Loss: 1.4471\n",
            "Epoch: 1/1... Step: 852... Loss: 1.2400... Val Loss: 1.4299\n",
            "Epoch: 1/1... Step: 853... Loss: 1.1085... Val Loss: 1.4195\n",
            "Epoch: 1/1... Step: 854... Loss: 1.2917... Val Loss: 1.4391\n",
            "Epoch: 1/1... Step: 855... Loss: 1.2100... Val Loss: 1.4527\n",
            "Epoch: 1/1... Step: 856... Loss: 1.1789... Val Loss: 1.4682\n",
            "Epoch: 1/1... Step: 857... Loss: 1.1390... Val Loss: 1.4751\n",
            "Epoch: 1/1... Step: 858... Loss: 1.2276... Val Loss: 1.4266\n",
            "Epoch: 1/1... Step: 859... Loss: 1.3101... Val Loss: 1.4571\n",
            "Epoch: 1/1... Step: 860... Loss: 1.3225... Val Loss: 1.4035\n",
            "Epoch: 1/1... Step: 861... Loss: 1.3336... Val Loss: 1.4496\n",
            "Epoch: 1/1... Step: 862... Loss: 1.3254... Val Loss: 1.4161\n",
            "Epoch: 1/1... Step: 863... Loss: 1.2857... Val Loss: 1.4541\n",
            "Epoch: 1/1... Step: 864... Loss: 1.2365... Val Loss: 1.4000\n",
            "Epoch: 1/1... Step: 865... Loss: 1.2391... Val Loss: 1.4289\n",
            "Epoch: 1/1... Step: 866... Loss: 1.3097... Val Loss: 1.3952\n",
            "Epoch: 1/1... Step: 867... Loss: 1.1990... Val Loss: 1.4735\n",
            "Epoch: 1/1... Step: 868... Loss: 1.2334... Val Loss: 1.4130\n",
            "Epoch: 1/1... Step: 869... Loss: 1.2330... Val Loss: 1.4343\n",
            "Epoch: 1/1... Step: 870... Loss: 1.2745... Val Loss: 1.4187\n",
            "Epoch: 1/1... Step: 871... Loss: 1.2411... Val Loss: 1.4321\n",
            "Epoch: 1/1... Step: 872... Loss: 1.4068... Val Loss: 1.4116\n",
            "Epoch: 1/1... Step: 873... Loss: 1.3457... Val Loss: 1.4344\n",
            "Epoch: 1/1... Step: 874... Loss: 1.2322... Val Loss: 1.4069\n",
            "Epoch: 1/1... Step: 875... Loss: 1.3769... Val Loss: 1.4198\n",
            "Epoch: 1/1... Step: 876... Loss: 1.2538... Val Loss: 1.3967\n",
            "Epoch: 1/1... Step: 877... Loss: 1.2914... Val Loss: 1.4320\n",
            "Epoch: 1/1... Step: 878... Loss: 1.1591... Val Loss: 1.4062\n",
            "Epoch: 1/1... Step: 879... Loss: 1.3713... Val Loss: 1.4310\n",
            "Epoch: 1/1... Step: 880... Loss: 1.3201... Val Loss: 1.4324\n",
            "Epoch: 1/1... Step: 881... Loss: 1.1778... Val Loss: 1.4060\n",
            "Epoch: 1/1... Step: 882... Loss: 1.1905... Val Loss: 1.4062\n",
            "Epoch: 1/1... Step: 883... Loss: 1.2633... Val Loss: 1.4025\n",
            "Epoch: 1/1... Step: 884... Loss: 1.2155... Val Loss: 1.4262\n",
            "Epoch: 1/1... Step: 885... Loss: 1.1813... Val Loss: 1.4328\n",
            "Epoch: 1/1... Step: 886... Loss: 1.2554... Val Loss: 1.4612\n",
            "Epoch: 1/1... Step: 887... Loss: 1.3318... Val Loss: 1.4280\n",
            "Epoch: 1/1... Step: 888... Loss: 1.1342... Val Loss: 1.4709\n",
            "Epoch: 1/1... Step: 889... Loss: 1.1512... Val Loss: 1.4401\n",
            "Epoch: 1/1... Step: 890... Loss: 1.2193... Val Loss: 1.4083\n",
            "Epoch: 1/1... Step: 891... Loss: 1.3484... Val Loss: 1.4240\n",
            "Epoch: 1/1... Step: 892... Loss: 1.2166... Val Loss: 1.4115\n",
            "Epoch: 1/1... Step: 893... Loss: 1.2232... Val Loss: 1.4406\n",
            "Epoch: 1/1... Step: 894... Loss: 1.1511... Val Loss: 1.4238\n",
            "Epoch: 1/1... Step: 895... Loss: 1.2320... Val Loss: 1.4147\n",
            "Epoch: 1/1... Step: 896... Loss: 1.3016... Val Loss: 1.3995\n",
            "Epoch: 1/1... Step: 897... Loss: 1.3608... Val Loss: 1.3906\n",
            "Epoch: 1/1... Step: 898... Loss: 1.2306... Val Loss: 1.4053\n",
            "Epoch: 1/1... Step: 899... Loss: 1.3816... Val Loss: 1.4247\n",
            "Epoch: 1/1... Step: 900... Loss: 1.0628... Val Loss: 1.4392\n",
            "Epoch: 1/1... Step: 901... Loss: 1.2073... Val Loss: 1.4335\n",
            "Epoch: 1/1... Step: 902... Loss: 1.1821... Val Loss: 1.4465\n",
            "Epoch: 1/1... Step: 903... Loss: 1.3331... Val Loss: 1.4249\n",
            "Epoch: 1/1... Step: 904... Loss: 1.2509... Val Loss: 1.4443\n",
            "Epoch: 1/1... Step: 905... Loss: 1.2048... Val Loss: 1.4158\n",
            "Epoch: 1/1... Step: 906... Loss: 1.2090... Val Loss: 1.4035\n",
            "Epoch: 1/1... Step: 907... Loss: 1.2844... Val Loss: 1.4160\n",
            "Epoch: 1/1... Step: 908... Loss: 1.2783... Val Loss: 1.4011\n",
            "Epoch: 1/1... Step: 909... Loss: 1.3932... Val Loss: 1.4271\n",
            "Epoch: 1/1... Step: 910... Loss: 1.1151... Val Loss: 1.4172\n",
            "Epoch: 1/1... Step: 911... Loss: 1.3067... Val Loss: 1.4340\n",
            "Epoch: 1/1... Step: 912... Loss: 1.1736... Val Loss: 1.4018\n",
            "Epoch: 1/1... Step: 913... Loss: 1.3398... Val Loss: 1.4127\n",
            "Epoch: 1/1... Step: 914... Loss: 1.1594... Val Loss: 1.4170\n",
            "Epoch: 1/1... Step: 915... Loss: 1.4704... Val Loss: 1.3877\n",
            "Epoch: 1/1... Step: 916... Loss: 1.3119... Val Loss: 1.3859\n",
            "Epoch: 1/1... Step: 917... Loss: 1.1446... Val Loss: 1.4043\n",
            "Epoch: 1/1... Step: 918... Loss: 1.1955... Val Loss: 1.4100\n",
            "Epoch: 1/1... Step: 919... Loss: 1.2745... Val Loss: 1.3971\n",
            "Epoch: 1/1... Step: 920... Loss: 1.2720... Val Loss: 1.4150\n",
            "Epoch: 1/1... Step: 921... Loss: 1.2871... Val Loss: 1.3812\n",
            "Epoch: 1/1... Step: 922... Loss: 1.2512... Val Loss: 1.4061\n",
            "Epoch: 1/1... Step: 923... Loss: 1.1822... Val Loss: 1.3988\n",
            "Epoch: 1/1... Step: 924... Loss: 1.1588... Val Loss: 1.4150\n",
            "Epoch: 1/1... Step: 925... Loss: 1.2209... Val Loss: 1.4109\n",
            "Epoch: 1/1... Step: 926... Loss: 1.0866... Val Loss: 1.4162\n",
            "Epoch: 1/1... Step: 927... Loss: 1.2262... Val Loss: 1.4147\n",
            "Epoch: 1/1... Step: 928... Loss: 1.3001... Val Loss: 1.3790\n",
            "Epoch: 1/1... Step: 929... Loss: 1.3715... Val Loss: 1.3921\n",
            "Epoch: 1/1... Step: 930... Loss: 1.1876... Val Loss: 1.3925\n",
            "Epoch: 1/1... Step: 931... Loss: 1.1759... Val Loss: 1.4156\n",
            "Epoch: 1/1... Step: 932... Loss: 1.2535... Val Loss: 1.3861\n",
            "Epoch: 1/1... Step: 933... Loss: 1.2035... Val Loss: 1.3918\n",
            "Epoch: 1/1... Step: 934... Loss: 1.1192... Val Loss: 1.3964\n",
            "Epoch: 1/1... Step: 935... Loss: 1.2503... Val Loss: 1.4006\n",
            "Epoch: 1/1... Step: 936... Loss: 1.2511... Val Loss: 1.4032\n",
            "Epoch: 1/1... Step: 937... Loss: 1.3590... Val Loss: 1.3865\n",
            "Epoch: 1/1... Step: 938... Loss: 1.1849... Val Loss: 1.4010\n",
            "Epoch: 1/1... Step: 939... Loss: 1.2414... Val Loss: 1.3999\n",
            "Epoch: 1/1... Step: 940... Loss: 1.2182... Val Loss: 1.4123\n",
            "Epoch: 1/1... Step: 941... Loss: 1.1996... Val Loss: 1.3740\n",
            "Epoch: 1/1... Step: 942... Loss: 1.2913... Val Loss: 1.3978\n",
            "Epoch: 1/1... Step: 943... Loss: 1.3598... Val Loss: 1.3593\n",
            "Epoch: 1/1... Step: 944... Loss: 1.1758... Val Loss: 1.3889\n",
            "Epoch: 1/1... Step: 945... Loss: 1.2121... Val Loss: 1.3547\n",
            "Epoch: 1/1... Step: 946... Loss: 1.1854... Val Loss: 1.4027\n",
            "Epoch: 1/1... Step: 947... Loss: 1.2439... Val Loss: 1.3776\n",
            "Epoch: 1/1... Step: 948... Loss: 1.2466... Val Loss: 1.4086\n",
            "Epoch: 1/1... Step: 949... Loss: 1.1595... Val Loss: 1.3992\n",
            "Epoch: 1/1... Step: 950... Loss: 1.1824... Val Loss: 1.4254\n",
            "Epoch: 1/1... Step: 951... Loss: 1.1304... Val Loss: 1.3934\n",
            "Epoch: 1/1... Step: 952... Loss: 1.2293... Val Loss: 1.4056\n",
            "Epoch: 1/1... Step: 953... Loss: 1.3046... Val Loss: 1.3906\n",
            "Epoch: 1/1... Step: 954... Loss: 1.3037... Val Loss: 1.4071\n",
            "Epoch: 1/1... Step: 955... Loss: 1.2789... Val Loss: 1.3967\n",
            "Epoch: 1/1... Step: 956... Loss: 1.1617... Val Loss: 1.3917\n",
            "Epoch: 1/1... Step: 957... Loss: 1.1425... Val Loss: 1.4218\n",
            "Epoch: 1/1... Step: 958... Loss: 1.2047... Val Loss: 1.4062\n",
            "Epoch: 1/1... Step: 959... Loss: 1.2048... Val Loss: 1.4239\n",
            "Epoch: 1/1... Step: 960... Loss: 1.2603... Val Loss: 1.3905\n",
            "Epoch: 1/1... Step: 961... Loss: 1.3920... Val Loss: 1.3868\n",
            "Epoch: 1/1... Step: 962... Loss: 1.2395... Val Loss: 1.3934\n",
            "Epoch: 1/1... Step: 963... Loss: 1.0946... Val Loss: 1.4083\n",
            "Epoch: 1/1... Step: 964... Loss: 1.3427... Val Loss: 1.3784\n",
            "Epoch: 1/1... Step: 965... Loss: 1.2922... Val Loss: 1.3682\n",
            "Epoch: 1/1... Step: 966... Loss: 1.2447... Val Loss: 1.3828\n",
            "Epoch: 1/1... Step: 967... Loss: 1.1335... Val Loss: 1.3809\n",
            "Epoch: 1/1... Step: 968... Loss: 1.1183... Val Loss: 1.3985\n",
            "Epoch: 1/1... Step: 969... Loss: 1.1352... Val Loss: 1.3479\n",
            "Epoch: 1/1... Step: 970... Loss: 1.2962... Val Loss: 1.4039\n",
            "Epoch: 1/1... Step: 971... Loss: 1.1280... Val Loss: 1.4017\n",
            "Epoch: 1/1... Step: 972... Loss: 1.2127... Val Loss: 1.4065\n",
            "Epoch: 1/1... Step: 973... Loss: 1.3775... Val Loss: 1.3879\n",
            "Epoch: 1/1... Step: 974... Loss: 1.2724... Val Loss: 1.3991\n",
            "Epoch: 1/1... Step: 975... Loss: 1.2533... Val Loss: 1.4161\n",
            "Epoch: 1/1... Step: 976... Loss: 0.9676... Val Loss: 1.3938\n",
            "Epoch: 1/1... Step: 977... Loss: 1.1639... Val Loss: 1.4020\n",
            "Epoch: 1/1... Step: 978... Loss: 1.0678... Val Loss: 1.3779\n",
            "Epoch: 1/1... Step: 979... Loss: 1.0837... Val Loss: 1.3981\n",
            "Epoch: 1/1... Step: 980... Loss: 1.1388... Val Loss: 1.4068\n",
            "Epoch: 1/1... Step: 981... Loss: 1.1922... Val Loss: 1.3841\n",
            "Epoch: 1/1... Step: 982... Loss: 1.3620... Val Loss: 1.4060\n",
            "Epoch: 1/1... Step: 983... Loss: 1.3849... Val Loss: 1.3867\n",
            "Epoch: 1/1... Step: 984... Loss: 1.2001... Val Loss: 1.4046\n",
            "Epoch: 1/1... Step: 985... Loss: 1.2257... Val Loss: 1.3970\n",
            "Epoch: 1/1... Step: 986... Loss: 1.2351... Val Loss: 1.3731\n",
            "Epoch: 1/1... Step: 987... Loss: 1.2221... Val Loss: 1.4098\n",
            "Epoch: 1/1... Step: 988... Loss: 1.2071... Val Loss: 1.3953\n",
            "Epoch: 1/1... Step: 989... Loss: 1.1802... Val Loss: 1.3899\n",
            "Epoch: 1/1... Step: 990... Loss: 1.2708... Val Loss: 1.4018\n",
            "Epoch: 1/1... Step: 991... Loss: 1.1939... Val Loss: 1.3698\n",
            "Epoch: 1/1... Step: 992... Loss: 1.2992... Val Loss: 1.4050\n",
            "Epoch: 1/1... Step: 993... Loss: 1.1704... Val Loss: 1.3654\n",
            "Epoch: 1/1... Step: 994... Loss: 1.4078... Val Loss: 1.3763\n",
            "Epoch: 1/1... Step: 995... Loss: 1.3254... Val Loss: 1.3836\n",
            "Epoch: 1/1... Step: 996... Loss: 1.2318... Val Loss: 1.3879\n",
            "Epoch: 1/1... Step: 997... Loss: 1.2808... Val Loss: 1.3940\n",
            "Epoch: 1/1... Step: 998... Loss: 1.2562... Val Loss: 1.3716\n",
            "Epoch: 1/1... Step: 999... Loss: 1.3094... Val Loss: 1.3716\n",
            "Epoch: 1/1... Step: 1000... Loss: 1.1937... Val Loss: 1.3712\n",
            "Epoch: 1/1... Step: 1001... Loss: 1.1603... Val Loss: 1.3841\n",
            "Epoch: 1/1... Step: 1002... Loss: 1.1217... Val Loss: 1.3650\n",
            "Epoch: 1/1... Step: 1003... Loss: 1.2678... Val Loss: 1.3788\n",
            "Epoch: 1/1... Step: 1004... Loss: 1.2442... Val Loss: 1.3702\n",
            "Epoch: 1/1... Step: 1005... Loss: 1.2528... Val Loss: 1.3827\n",
            "Epoch: 1/1... Step: 1006... Loss: 1.1901... Val Loss: 1.3723\n",
            "Epoch: 1/1... Step: 1007... Loss: 1.1310... Val Loss: 1.3958\n",
            "Epoch: 1/1... Step: 1008... Loss: 1.1998... Val Loss: 1.3646\n",
            "Epoch: 1/1... Step: 1009... Loss: 1.2538... Val Loss: 1.3658\n",
            "Epoch: 1/1... Step: 1010... Loss: 1.1973... Val Loss: 1.3860\n",
            "Epoch: 1/1... Step: 1011... Loss: 1.0517... Val Loss: 1.3946\n",
            "Epoch: 1/1... Step: 1012... Loss: 1.2042... Val Loss: 1.3830\n",
            "Epoch: 1/1... Step: 1013... Loss: 1.1758... Val Loss: 1.3623\n",
            "Epoch: 1/1... Step: 1014... Loss: 1.2651... Val Loss: 1.3585\n",
            "Epoch: 1/1... Step: 1015... Loss: 1.3096... Val Loss: 1.3648\n",
            "Epoch: 1/1... Step: 1016... Loss: 1.2280... Val Loss: 1.3468\n",
            "Epoch: 1/1... Step: 1017... Loss: 1.1616... Val Loss: 1.3987\n",
            "Epoch: 1/1... Step: 1018... Loss: 1.1901... Val Loss: 1.3652\n",
            "Epoch: 1/1... Step: 1019... Loss: 1.1628... Val Loss: 1.3963\n",
            "Epoch: 1/1... Step: 1020... Loss: 1.2253... Val Loss: 1.3446\n",
            "Epoch: 1/1... Step: 1021... Loss: 1.3696... Val Loss: 1.3775\n",
            "Epoch: 1/1... Step: 1022... Loss: 1.1701... Val Loss: 1.3592\n",
            "Epoch: 1/1... Step: 1023... Loss: 1.3094... Val Loss: 1.3759\n",
            "Epoch: 1/1... Step: 1024... Loss: 1.2609... Val Loss: 1.3653\n",
            "Epoch: 1/1... Step: 1025... Loss: 1.3404... Val Loss: 1.3742\n",
            "Epoch: 1/1... Step: 1026... Loss: 1.2592... Val Loss: 1.3584\n",
            "Epoch: 1/1... Step: 1027... Loss: 1.1556... Val Loss: 1.3707\n",
            "Epoch: 1/1... Step: 1028... Loss: 1.0732... Val Loss: 1.3495\n",
            "Epoch: 1/1... Step: 1029... Loss: 1.2001... Val Loss: 1.3883\n",
            "Epoch: 1/1... Step: 1030... Loss: 1.2398... Val Loss: 1.3565\n",
            "Epoch: 1/1... Step: 1031... Loss: 1.2408... Val Loss: 1.3978\n",
            "Epoch: 1/1... Step: 1032... Loss: 1.1196... Val Loss: 1.3902\n",
            "Epoch: 1/1... Step: 1033... Loss: 1.2427... Val Loss: 1.4162\n",
            "Epoch: 1/1... Step: 1034... Loss: 1.2107... Val Loss: 1.3614\n",
            "Epoch: 1/1... Step: 1035... Loss: 1.3408... Val Loss: 1.3835\n",
            "Epoch: 1/1... Step: 1036... Loss: 1.3065... Val Loss: 1.3638\n",
            "Epoch: 1/1... Step: 1037... Loss: 1.2624... Val Loss: 1.3693\n",
            "Epoch: 1/1... Step: 1038... Loss: 1.2524... Val Loss: 1.3701\n",
            "Epoch: 1/1... Step: 1039... Loss: 1.3314... Val Loss: 1.3716\n",
            "Epoch: 1/1... Step: 1040... Loss: 1.2062... Val Loss: 1.3817\n",
            "Epoch: 1/1... Step: 1041... Loss: 1.2276... Val Loss: 1.3871\n",
            "Epoch: 1/1... Step: 1042... Loss: 1.2045... Val Loss: 1.3622\n",
            "Epoch: 1/1... Step: 1043... Loss: 1.1476... Val Loss: 1.3852\n",
            "Epoch: 1/1... Step: 1044... Loss: 1.1308... Val Loss: 1.3571\n",
            "Epoch: 1/1... Step: 1045... Loss: 1.2584... Val Loss: 1.3606\n",
            "Epoch: 1/1... Step: 1046... Loss: 1.1862... Val Loss: 1.3778\n",
            "Epoch: 1/1... Step: 1047... Loss: 1.0994... Val Loss: 1.3686\n",
            "Epoch: 1/1... Step: 1048... Loss: 1.1933... Val Loss: 1.3375\n",
            "Epoch: 1/1... Step: 1049... Loss: 1.3559... Val Loss: 1.3361\n",
            "Epoch: 1/1... Step: 1050... Loss: 1.2481... Val Loss: 1.3505\n",
            "Epoch: 1/1... Step: 1051... Loss: 1.1931... Val Loss: 1.3575\n",
            "Epoch: 1/1... Step: 1052... Loss: 1.2291... Val Loss: 1.3606\n",
            "Epoch: 1/1... Step: 1053... Loss: 1.1569... Val Loss: 1.3643\n",
            "Epoch: 1/1... Step: 1054... Loss: 1.2214... Val Loss: 1.3584\n",
            "Epoch: 1/1... Step: 1055... Loss: 1.2319... Val Loss: 1.3587\n",
            "Epoch: 1/1... Step: 1056... Loss: 1.2501... Val Loss: 1.3714\n",
            "Epoch: 1/1... Step: 1057... Loss: 1.3145... Val Loss: 1.3696\n",
            "Epoch: 1/1... Step: 1058... Loss: 1.2948... Val Loss: 1.3542\n",
            "Epoch: 1/1... Step: 1059... Loss: 1.2355... Val Loss: 1.3778\n",
            "Epoch: 1/1... Step: 1060... Loss: 1.1524... Val Loss: 1.3605\n",
            "Epoch: 1/1... Step: 1061... Loss: 1.1762... Val Loss: 1.3653\n",
            "Epoch: 1/1... Step: 1062... Loss: 1.2725... Val Loss: 1.3625\n",
            "Epoch: 1/1... Step: 1063... Loss: 1.1320... Val Loss: 1.3625\n",
            "Epoch: 1/1... Step: 1064... Loss: 1.2342... Val Loss: 1.3748\n",
            "Epoch: 1/1... Step: 1065... Loss: 1.1705... Val Loss: 1.3496\n",
            "Epoch: 1/1... Step: 1066... Loss: 1.3052... Val Loss: 1.3465\n",
            "Epoch: 1/1... Step: 1067... Loss: 1.2268... Val Loss: 1.3490\n",
            "Epoch: 1/1... Step: 1068... Loss: 1.1324... Val Loss: 1.3658\n",
            "Epoch: 1/1... Step: 1069... Loss: 1.2355... Val Loss: 1.3419\n",
            "Epoch: 1/1... Step: 1070... Loss: 1.1656... Val Loss: 1.3470\n",
            "Epoch: 1/1... Step: 1071... Loss: 1.2004... Val Loss: 1.3641\n",
            "Epoch: 1/1... Step: 1072... Loss: 1.1629... Val Loss: 1.3414\n",
            "Epoch: 1/1... Step: 1073... Loss: 1.2240... Val Loss: 1.3839\n",
            "Epoch: 1/1... Step: 1074... Loss: 1.2520... Val Loss: 1.3118\n",
            "Epoch: 1/1... Step: 1075... Loss: 1.2473... Val Loss: 1.3557\n",
            "Epoch: 1/1... Step: 1076... Loss: 1.1684... Val Loss: 1.3451\n",
            "Epoch: 1/1... Step: 1077... Loss: 1.1908... Val Loss: 1.3395\n",
            "Epoch: 1/1... Step: 1078... Loss: 1.2738... Val Loss: 1.3467\n",
            "Epoch: 1/1... Step: 1079... Loss: 1.3109... Val Loss: 1.3277\n",
            "Epoch: 1/1... Step: 1080... Loss: 1.3489... Val Loss: 1.3489\n",
            "Epoch: 1/1... Step: 1081... Loss: 1.1935... Val Loss: 1.3372\n",
            "Epoch: 1/1... Step: 1082... Loss: 1.1551... Val Loss: 1.3510\n",
            "Epoch: 1/1... Step: 1083... Loss: 1.2627... Val Loss: 1.3436\n",
            "Epoch: 1/1... Step: 1084... Loss: 1.1140... Val Loss: 1.3469\n",
            "Epoch: 1/1... Step: 1085... Loss: 1.1388... Val Loss: 1.3642\n",
            "Epoch: 1/1... Step: 1086... Loss: 1.1010... Val Loss: 1.3517\n",
            "Epoch: 1/1... Step: 1087... Loss: 1.1533... Val Loss: 1.3393\n",
            "Epoch: 1/1... Step: 1088... Loss: 1.2633... Val Loss: 1.3160\n",
            "Epoch: 1/1... Step: 1089... Loss: 1.1900... Val Loss: 1.3799\n",
            "Epoch: 1/1... Step: 1090... Loss: 1.1790... Val Loss: 1.3256\n",
            "Epoch: 1/1... Step: 1091... Loss: 1.1963... Val Loss: 1.3751\n",
            "Epoch: 1/1... Step: 1092... Loss: 1.0868... Val Loss: 1.3473\n",
            "Epoch: 1/1... Step: 1093... Loss: 1.3337... Val Loss: 1.3541\n",
            "Epoch: 1/1... Step: 1094... Loss: 1.1210... Val Loss: 1.3534\n",
            "Epoch: 1/1... Step: 1095... Loss: 1.2330... Val Loss: 1.3487\n",
            "Epoch: 1/1... Step: 1096... Loss: 1.1724... Val Loss: 1.3340\n",
            "Epoch: 1/1... Step: 1097... Loss: 1.1674... Val Loss: 1.3464\n",
            "Epoch: 1/1... Step: 1098... Loss: 1.2371... Val Loss: 1.3260\n",
            "Epoch: 1/1... Step: 1099... Loss: 1.2066... Val Loss: 1.3678\n",
            "Epoch: 1/1... Step: 1100... Loss: 1.1730... Val Loss: 1.3274\n",
            "Epoch: 1/1... Step: 1101... Loss: 1.3092... Val Loss: 1.3381\n",
            "Epoch: 1/1... Step: 1102... Loss: 1.2758... Val Loss: 1.3665\n",
            "Epoch: 1/1... Step: 1103... Loss: 1.1090... Val Loss: 1.3569\n",
            "Epoch: 1/1... Step: 1104... Loss: 1.1252... Val Loss: 1.3684\n",
            "Epoch: 1/1... Step: 1105... Loss: 1.2445... Val Loss: 1.3711\n",
            "Epoch: 1/1... Step: 1106... Loss: 1.0974... Val Loss: 1.4027\n",
            "Epoch: 1/1... Step: 1107... Loss: 1.2855... Val Loss: 1.3657\n",
            "Epoch: 1/1... Step: 1108... Loss: 1.1539... Val Loss: 1.3583\n",
            "Epoch: 1/1... Step: 1109... Loss: 1.3473... Val Loss: 1.3417\n",
            "Epoch: 1/1... Step: 1110... Loss: 1.0539... Val Loss: 1.3944\n",
            "Epoch: 1/1... Step: 1111... Loss: 1.1943... Val Loss: 1.3263\n",
            "Epoch: 1/1... Step: 1112... Loss: 1.1160... Val Loss: 1.3598\n",
            "Epoch: 1/1... Step: 1113... Loss: 1.1711... Val Loss: 1.3383\n",
            "Epoch: 1/1... Step: 1114... Loss: 1.2292... Val Loss: 1.3418\n",
            "Epoch: 1/1... Step: 1115... Loss: 1.3464... Val Loss: 1.3475\n",
            "Epoch: 1/1... Step: 1116... Loss: 1.1537... Val Loss: 1.3529\n",
            "Epoch: 1/1... Step: 1117... Loss: 1.3680... Val Loss: 1.3482\n",
            "Epoch: 1/1... Step: 1118... Loss: 1.1921... Val Loss: 1.3510\n",
            "Epoch: 1/1... Step: 1119... Loss: 1.2044... Val Loss: 1.3290\n",
            "Epoch: 1/1... Step: 1120... Loss: 1.2224... Val Loss: 1.3361\n",
            "Epoch: 1/1... Step: 1121... Loss: 1.2281... Val Loss: 1.3448\n",
            "Epoch: 1/1... Step: 1122... Loss: 1.2774... Val Loss: 1.3689\n",
            "Epoch: 1/1... Step: 1123... Loss: 1.3766... Val Loss: 1.3010\n",
            "Epoch: 1/1... Step: 1124... Loss: 1.1907... Val Loss: 1.3440\n",
            "Epoch: 1/1... Step: 1125... Loss: 1.3782... Val Loss: 1.3101\n",
            "Epoch: 1/1... Step: 1126... Loss: 1.1880... Val Loss: 1.3313\n",
            "Epoch: 1/1... Step: 1127... Loss: 1.0993... Val Loss: 1.3493\n",
            "Epoch: 1/1... Step: 1128... Loss: 1.1077... Val Loss: 1.3381\n",
            "Epoch: 1/1... Step: 1129... Loss: 1.2251... Val Loss: 1.3474\n",
            "Epoch: 1/1... Step: 1130... Loss: 1.1268... Val Loss: 1.3381\n",
            "Epoch: 1/1... Step: 1131... Loss: 1.2191... Val Loss: 1.3199\n",
            "Epoch: 1/1... Step: 1132... Loss: 1.1768... Val Loss: 1.3117\n",
            "Epoch: 1/1... Step: 1133... Loss: 1.2551... Val Loss: 1.3079\n",
            "Epoch: 1/1... Step: 1134... Loss: 1.0911... Val Loss: 1.3308\n",
            "Epoch: 1/1... Step: 1135... Loss: 1.1197... Val Loss: 1.2956\n",
            "Epoch: 1/1... Step: 1136... Loss: 1.2672... Val Loss: 1.3233\n",
            "Epoch: 1/1... Step: 1137... Loss: 1.2014... Val Loss: 1.3027\n",
            "Epoch: 1/1... Step: 1138... Loss: 1.2407... Val Loss: 1.3315\n",
            "Epoch: 1/1... Step: 1139... Loss: 1.2601... Val Loss: 1.3251\n",
            "Epoch: 1/1... Step: 1140... Loss: 1.1251... Val Loss: 1.3536\n",
            "Epoch: 1/1... Step: 1141... Loss: 1.1626... Val Loss: 1.3388\n",
            "Epoch: 1/1... Step: 1142... Loss: 1.1118... Val Loss: 1.3602\n",
            "Epoch: 1/1... Step: 1143... Loss: 1.1556... Val Loss: 1.3129\n",
            "Epoch: 1/1... Step: 1144... Loss: 1.1239... Val Loss: 1.3440\n",
            "Epoch: 1/1... Step: 1145... Loss: 1.2554... Val Loss: 1.3208\n",
            "Epoch: 1/1... Step: 1146... Loss: 1.1484... Val Loss: 1.3527\n",
            "Epoch: 1/1... Step: 1147... Loss: 1.2424... Val Loss: 1.3191\n",
            "Epoch: 1/1... Step: 1148... Loss: 1.2202... Val Loss: 1.3608\n",
            "Epoch: 1/1... Step: 1149... Loss: 1.1496... Val Loss: 1.3179\n",
            "Epoch: 1/1... Step: 1150... Loss: 1.3587... Val Loss: 1.3371\n",
            "Epoch: 1/1... Step: 1151... Loss: 1.1966... Val Loss: 1.3145\n",
            "Epoch: 1/1... Step: 1152... Loss: 1.3053... Val Loss: 1.3150\n",
            "Epoch: 1/1... Step: 1153... Loss: 1.2075... Val Loss: 1.3599\n",
            "Epoch: 1/1... Step: 1154... Loss: 1.2172... Val Loss: 1.3422\n",
            "Epoch: 1/1... Step: 1155... Loss: 1.0981... Val Loss: 1.3187\n",
            "Epoch: 1/1... Step: 1156... Loss: 1.4420... Val Loss: 1.3430\n"
          ]
        }
      ],
      "source": [
        "\n",
        "batch_size = 64\n",
        "window_size = 50\n",
        "n_epochs = 1\n",
        "\n",
        "LM.train_char_LSTM(LM,Train_dp.encoded_data, epochs=n_epochs,\n",
        "                   batch_size=batch_size, window_size=window_size, print_every=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t8QEpGzna8a"
      },
      "source": [
        "## Get_probs() Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I2z-0vx_Xrd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2f8bc87-0550-4457-b7e3-d64ad9699cd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.00032162736: 'e',\n",
              " 0.00040367394: 'N',\n",
              " 0.0005101248: 'ژ',\n",
              " 0.0006548689: 's',\n",
              " 0.0008097274: 'آ',\n",
              " 0.0008275509: 'چ',\n",
              " 0.0009423145: 'ظ',\n",
              " 0.0012082964: 'ذ',\n",
              " 0.0013955449: 'گ',\n",
              " 0.0016241978: 'ض',\n",
              " 0.0017000367: 'ث',\n",
              " 0.0017614838: 'غ',\n",
              " 0.0018540353: 'و',\n",
              " 0.0020526147: 'ا',\n",
              " 0.0023990504: 'ق',\n",
              " 0.0034291463: 'ح',\n",
              " 0.0043661967: 'ج',\n",
              " 0.0049889763: 'پ',\n",
              " 0.005061446: 'ک',\n",
              " 0.0061951657: 'ت',\n",
              " 0.0073631895: 'ف',\n",
              " 0.008025317: 'ط',\n",
              " 0.008182397: 'ع',\n",
              " 0.008707118: 'ص',\n",
              " 0.00946943: 'خ',\n",
              " 0.016344521: ' ',\n",
              " 0.019773483: 'ز',\n",
              " 0.019938922: 'ب',\n",
              " 0.021542968: 'ش',\n",
              " 0.021917198: 'ه',\n",
              " 0.023191046: 'س',\n",
              " 0.026945792: 'ل',\n",
              " 0.029691327: 'د',\n",
              " 0.043907925: 'م',\n",
              " 0.060801726: 'ی',\n",
              " 0.30166188: 'ن',\n",
              " 0.33002967: 'ر'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "LM.get_probs(LM, prefix=\"انتخا\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tg0tc5UqXzPm",
        "outputId": "5c1d2c48-2b8e-4f19-9957-909fcecdc9ef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.00015371614: 's',\n",
              " 0.00018961634: 'e',\n",
              " 0.0002590321: 'ا',\n",
              " 0.00026481587: 'چ',\n",
              " 0.00029250453: 'N',\n",
              " 0.00032283276: 'ذ',\n",
              " 0.00040296314: 'ژ',\n",
              " 0.0004934624: 'غ',\n",
              " 0.00057173835: 'آ',\n",
              " 0.0006586413: 'ظ',\n",
              " 0.0007182893: 'ش',\n",
              " 0.0008499607: 'ص',\n",
              " 0.0009408724: 'ض',\n",
              " 0.00096357823: 'گ',\n",
              " 0.0010058681: 'پ',\n",
              " 0.0011666031: 'ط',\n",
              " 0.0012120103: 'ث',\n",
              " 0.0013168021: 'خ',\n",
              " 0.0014200592: 'ر',\n",
              " 0.0014988626: 'ج',\n",
              " 0.001524645: 'و',\n",
              " 0.0017630483: 'ح',\n",
              " 0.0030081237: 'ک',\n",
              " 0.0032516997: 'ع',\n",
              " 0.0045482498: 'ف',\n",
              " 0.00480885: 'ق',\n",
              " 0.006762034: 'ه',\n",
              " 0.008542996: 'ز',\n",
              " 0.009105222: 'ب',\n",
              " 0.014887797: 'س',\n",
              " 0.036302984: 'ت',\n",
              " 0.037912138: 'ی',\n",
              " 0.04678705: 'ل',\n",
              " 0.060584355: ' ',\n",
              " 0.06868335: 'ن',\n",
              " 0.06993459: 'د',\n",
              " 0.6068907: 'م'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "LM.get_probs(LM, prefix=\"سلا\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Xk0V2v4unQFC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f20a76d-d8a3-4f95-90ef-61f6ff76a6f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.0002243729: 'e',\n",
              " 0.00034124768: 'N',\n",
              " 0.00040384138: 'ث',\n",
              " 0.00040544922: 's',\n",
              " 0.00045313936: 'ظ',\n",
              " 0.0004721463: 'ژ',\n",
              " 0.00050417363: 'ذ',\n",
              " 0.0005377796: 'غ',\n",
              " 0.00067939673: 'آ',\n",
              " 0.0007971036: 'ص',\n",
              " 0.0011355467: 'ط',\n",
              " 0.0014944783: 'چ',\n",
              " 0.0015656585: 'ح',\n",
              " 0.0019000223: 'ق',\n",
              " 0.0019904638: 'ض',\n",
              " 0.002163194: 'خ',\n",
              " 0.0023262848: 'پ',\n",
              " 0.0029464588: 'ج',\n",
              " 0.0030250966: 'ش',\n",
              " 0.0030842773: 'ب',\n",
              " 0.0036089234: 'گ',\n",
              " 0.0038250214: 'ف',\n",
              " 0.0041364436: 'ع',\n",
              " 0.0062509147: 'س',\n",
              " 0.007384671: 'د',\n",
              " 0.0076091113: 'ز',\n",
              " 0.010452363: 'و',\n",
              " 0.011546834: 'ک',\n",
              " 0.017539252: 'ه',\n",
              " 0.019256664: 'م',\n",
              " 0.022767002: 'ن',\n",
              " 0.026755732: 'ر',\n",
              " 0.029261164: 'ل',\n",
              " 0.11778881: 'ا',\n",
              " 0.12442104: 'ی',\n",
              " 0.25396848: 'ت',\n",
              " 0.3069774: ' '}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "LM.get_probs(LM, prefix=\"فوتبالیس\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwhVzuGKnlBn"
      },
      "source": [
        "## Get_Next_Char() Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PFGrb7KtpCK-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5aa3d684-4e42-4593-9f5d-0c8db490417e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ن'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "LM.get_next_char(LM, prefix=\"انتخا\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wpaXch0npB65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fff2407c-abce-4951-c90e-38be9dadb299"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'د'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "LM.get_next_char(LM, prefix=\"سلا\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4bMpAJAdnkaV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b248be51-c217-4bef-c34a-d81d621bd161"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ت'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "LM.get_next_char(LM, prefix=\"فوتبالیس\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GqlATaUYpUtX"
      },
      "source": [
        "## Generate_text() Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "C1G72KzJpJOq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "dfce2b99-52fd-47cf-fd47-3cd9403d338c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'انتخار از ایر    بهگزارش خارههایین روز با ان اینکهاe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "LM.generate_text(LM, prefix=\"انتخا\",k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EWIUxhSrpI9h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "489faa08-2a6c-4863-8e21-18e15309632f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'سلاد مرام دار در گزارشییا از مالی ازار از ایرانیه بe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "LM.generate_text(LM, prefix=\"سلا\",k=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4ffwVx_lpIro",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c7ba8b4-8d35-44a6-b6d0-f4149e0412ff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'فوتبالیست داد ایر در گفتگفیبه خب    محددات مارو دریe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "LM.generate_text(LM, prefix=\"فوتبالیس\",k=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y-tGpPxqDzb"
      },
      "source": [
        "## get_overall_prob() Sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "h600_qgDqLEk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e9bce93-22bf-4c2c-ea51-f91baa081def"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-89.25869405269623"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "LM.get_overall_prob(LM, sentence=\"علی با تاکسی به مدرسه رفت e\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fQx91UdNqLEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb2794cc-04c7-41ec-90d2-191ae33314d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-116.04135018587112"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "LM.get_overall_prob(LM, sentence=\"سلام آقای اصفهانی حال شما چطوره e\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jIlT8A_cqLEm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7241ca5b-15b9-4b5a-db47-71ab9191e472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-111.31501233577728"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "LM.get_overall_prob(LM, sentence=\"مجتبی فوتبالیست خیلی خوبی است e\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "Q59arDF03T9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dp = DataProcessor('News/test.csv',start=0,end=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYMd2c7Z3Gbn",
        "outputId": "2c9ab21b-db30-4146-bd3a-ba18ed856a4c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- read_data() Done --------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  11835111\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- tokenize() Done --------------\n",
            "-------------- encoding() Done --------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LM.evaluate(LM, test_dp.data[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuYTP7qU3aDG",
        "outputId": "6c1675e6-52b0-4dc5-fd91-ea097e39824d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:181: RuntimeWarning: overflow encountered in exp\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9475.158883678117, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate ALL Char"
      ],
      "metadata": {
        "id": "Sje-mbTvL62R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoicfTEiAd_J",
        "outputId": "2d0b7209-8441-43ab-c5c5-e4bac850fd7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- read_data() Done --------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  21135596\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  15571938\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  13460989\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  13503170\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  18942407\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  22847197\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  14932993\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  16020616\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  11003477\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  9305916\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n"
          ]
        }
      ],
      "source": [
        "total_char = 0\n",
        "dp1 = DataProcessor('News/train.csv',start=0,end=10)\n",
        "total_char += dp1.n_chars\n",
        "del dp1\n",
        "dp2 = DataProcessor('News/train.csv',start=10,end=20)\n",
        "total_char += dp2.n_chars\n",
        "del dp2\n",
        "dp3 = DataProcessor('News/train.csv',start=20,end=30)\n",
        "total_char += dp3.n_chars\n",
        "del dp3\n",
        "dp4 = DataProcessor('News/train.csv',start=30,end=40)\n",
        "total_char += dp4.n_chars\n",
        "del dp4\n",
        "dp5 = DataProcessor('News/train.csv',start=40,end=50)\n",
        "total_char += dp5.n_chars\n",
        "del dp5\n",
        "dp6 = DataProcessor('News/train.csv',start=50,end=60)\n",
        "total_char += dp6.n_chars\n",
        "del dp6\n",
        "dp7 = DataProcessor('News/train.csv',start=60,end=70)\n",
        "total_char += dp7.n_chars\n",
        "del dp7\n",
        "dp8 = DataProcessor('News/train.csv',start=70,end=80)\n",
        "total_char += dp8.n_chars\n",
        "del dp8\n",
        "dp9 = DataProcessor('News/train.csv',start=80,end=90)\n",
        "total_char += dp9.n_chars\n",
        "del dp9\n",
        "dp10 = DataProcessor('News/train.csv',start=90,end=100)\n",
        "total_char += dp10.n_chars\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GS9WB_f9EG-0",
        "outputId": "f9262bb4-b605-493b-b44d-b703bdb1289e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total number of characters : 156724299\n",
            "total number of unique characters : 37\n"
          ]
        }
      ],
      "source": [
        "print('total number of characters :',total_char)\n",
        "print('total number of unique characters :',dp10.n_unique_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CP6nzsB8Fxkt",
        "outputId": "9bdfc92d-5137-4a2a-f595-65a341f27c30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- read_data() Done --------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  3799445\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  3075899\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  2534251\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  2425516\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  3316766\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  3893809\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  2740158\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  2984264\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  1869989\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n",
            "-------------- read_data() Done --------------\n",
            "-------------- clean_text() Done --------------\n",
            "Number of all characters:  1674295\n",
            "Number of unique characters:  37\n",
            "-------------- count_chars() Done --------------\n"
          ]
        }
      ],
      "source": [
        "total_char = 0\n",
        "dp1 = DataProcessor('News/test.csv',start=0,end=10)\n",
        "total_char += dp1.n_chars\n",
        "del dp1\n",
        "dp2 = DataProcessor('News/test.csv',start=10,end=20)\n",
        "total_char += dp2.n_chars\n",
        "del dp2\n",
        "dp3 = DataProcessor('News/test.csv',start=20,end=30)\n",
        "total_char += dp3.n_chars\n",
        "del dp3\n",
        "dp4 = DataProcessor('News/test.csv',start=30,end=40)\n",
        "total_char += dp4.n_chars\n",
        "del dp4\n",
        "dp5 = DataProcessor('News/test.csv',start=40,end=50)\n",
        "total_char += dp5.n_chars\n",
        "del dp5\n",
        "dp6 = DataProcessor('News/test.csv',start=50,end=60)\n",
        "total_char += dp6.n_chars\n",
        "del dp6\n",
        "dp7 = DataProcessor('News/test.csv',start=60,end=70)\n",
        "total_char += dp7.n_chars\n",
        "del dp7\n",
        "dp8 = DataProcessor('News/test.csv',start=70,end=80)\n",
        "total_char += dp8.n_chars\n",
        "del dp8\n",
        "dp9 = DataProcessor('News/test.csv',start=80,end=90)\n",
        "total_char += dp9.n_chars\n",
        "del dp9\n",
        "dp10 = DataProcessor('News/test.csv',start=90,end=100)\n",
        "total_char += dp10.n_chars\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa4fg3V5wMSk",
        "outputId": "9a1afb1e-b458-4834-9d8c-48faa0791073"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total number of characters : 28314392\n",
            "total number of unique characters : 37\n"
          ]
        }
      ],
      "source": [
        "print('total number of characters :',total_char)\n",
        "print('total number of unique characters :',dp10.n_unique_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkK-t9pEwbze"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP_Ex3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}